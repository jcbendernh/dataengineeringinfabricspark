{"cells":[{"cell_type":"markdown","source":["#### Dynamic Schema Evolution Handling & Data Quality Enforcement using Patient Encounter Data for the HealthcareData Lakehouse.\n","\n","Goal: Demonstrate how Fabric Spark Notebooks can handle incoming data with evolving schemas (new columns) while dynamically applying data quality (DQ) rules defined externally, showcasing Delta Lake's capabilities.\n","\n","Techniques to Showcase:\n","\n","    1. Defining DQ rules externally (using JSON in this demo).\n","    2. Writing a dynamic PySpark function to apply rules based on definitions and current DataFrame schema.\n","    3. Using Delta Lake CHECK constraints for simple, static rules enforced at write time.\n","    4. Using Delta Lake schema evolution (mergeSchema=true) to handle new columns in source data gracefully during appends.\n","    5. Applying the dynamic DQ function to the evolved table.\n","    6. Analyzing and reporting DQ failures.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b138d166-5635-4b4d-8ccd-fe1daee3d4d9"},{"cell_type":"markdown","source":["###### Reset Demo"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ef74cc2-c7d7-4ccc-adca-b8c450685d95"},{"cell_type":"code","source":["%%sql\n","\n","DROP TABLE IF EXISTS patient_encounters_dq_results;\n","DROP TABLE IF EXISTS patient_encounters_dynamic;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[2,3],"state":"finished","livy_statement_state":"available","session_id":"0f594dd2-53ad-41f8-b164-a7990180b401","normalized_state":"finished","queued_time":"2025-04-01T14:18:08.5439264Z","session_start_time":"2025-04-01T14:18:08.5442377Z","execution_start_time":"2025-04-01T14:18:14.5260075Z","execution_finish_time":"2025-04-01T14:18:29.8501831Z","parent_msg_id":"0f75686c-5e93-4b8e-bfb5-c980a1f358cc"},"text/plain":"StatementMeta(, 0f594dd2-53ad-41f8-b164-a7990180b401, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":1,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":1,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"45ad7537-147a-4611-a4a3-5b44c1195edf"},{"cell_type":"markdown","source":["#### Cell 1: Setup & Imports"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc3b9f51-9a69-4578-9f47-1ddfeca9f922"},{"cell_type":"code","source":["import json\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType, IntegerType, DateType, DecimalType, StructType, StructField\n","# Optional: from delta.tables import DeltaTable\n","\n","print(\"Setup complete. Imports loaded.\")\n","\n","# --- Configuration ---\n","landing_zone_path = \"Files/landing/healthcare\"\n","rules_json_path = f\"{landing_zone_path}/dq_rules.json\"\n","batch1_csv_path = f\"{landing_zone_path}/encounters_batch_1.csv\"\n","batch2_csv_path = f\"{landing_zone_path}/encounters_batch_2.csv\"\n","\n","# Define Delta table names within the Lakehouse ('Tables' folder)\n","# Assumes Lakehouse 'HealthcareData' is default or specified\n","encounter_table = \"patient_encounters_dynamic\"\n","dq_results_table = \"patient_encounters_dq_results\"\n","\n","print(f\"Rules Path: {rules_json_path}\")\n","print(f\"Batch 1 Path: {batch1_csv_path}\")\n","print(f\"Batch 2 Path: {batch2_csv_path}\")\n","print(f\"Encounter Table: {encounter_table}\")\n","print(f\"DQ Results Table: {dq_results_table}\")\n","\n","# Define today's date for checks (using current_date directly in function is better)\n","# today_date = F.current_date() # Get this inside the function"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"0f594dd2-53ad-41f8-b164-a7990180b401","normalized_state":"finished","queued_time":"2025-04-01T14:18:32.0195371Z","session_start_time":null,"execution_start_time":"2025-04-01T14:18:36.2275674Z","execution_finish_time":"2025-04-01T14:18:36.6269576Z","parent_msg_id":"5c262472-b80a-45ea-973f-607910227a50"},"text/plain":"StatementMeta(, 0f594dd2-53ad-41f8-b164-a7990180b401, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Setup complete. Imports loaded.\nRules Path: Files/landing/healthcare/dq_rules.json\nBatch 1 Path: Files/landing/healthcare/encounters_batch_1.csv\nBatch 2 Path: Files/landing/healthcare/encounters_batch_2.csv\nEncounter Table: patient_encounters_dynamic\nDQ Results Table: patient_encounters_dq_results\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e79a6b59-3284-4362-b0ec-09603b8f0f85"},{"cell_type":"markdown","source":["#### Cell 2: Define Dynamic DQ Check Function"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"14145905-ebca-4eca-a3b8-294a07355714"},{"cell_type":"code","source":["def apply_dynamic_dq(df, rules_list):\n","    \"\"\"\n","    Applies data quality rules defined in rules_list to a DataFrame.\n","\n","    Args:\n","        df (DataFrame): The input Spark DataFrame.\n","        rules_list (list): A list of dictionaries, where each dict defines a DQ rule.\n","\n","    Returns:\n","        DataFrame: The input DataFrame with additional columns (DQ_RuleID)\n","                   indicating the pass/fail status (True/False) for each active rule\n","                   applicable to the DataFrame's schema.\n","    \"\"\"\n","    df_schema = df.schema\n","    df_cols = df_schema.fieldNames()\n","    dq_exprs = [] # List to hold generated DQ check expressions\n","\n","    print(f\"Applying {len(rules_list)} DQ rules dynamically...\")\n","\n","    for rule in rules_list:\n","        rule_id = rule.get(\"rule_id\", \"UNKNOWN_RULE\")\n","        col_name = rule.get(\"column_name\")\n","        check_type = rule.get(\"check_type\")\n","        is_active = rule.get(\"is_active\", False)\n","        dq_col_name = f\"DQ_{rule_id}\" # Name of the resulting boolean column\n","\n","        if not is_active:\n","            print(f\"  Skipping inactive rule: {rule_id}\")\n","            continue\n","\n","        if not col_name or not check_type:\n","            print(f\"  Skipping invalid rule definition: {rule_id} (missing column_name or check_type)\")\n","            continue\n","\n","        # --- Check if the column exists in the DataFrame ---\n","        if col_name not in df_cols:\n","            print(f\"  Skipping rule '{rule_id}': Column '{col_name}' not found in DataFrame schema.\")\n","            continue\n","\n","        # --- Build the Spark SQL expression for the check ---\n","        # Default to True (passes) if rule logic has issue or column is null when check shouldn't apply to null\n","        check_expr = F.lit(True)\n","        try:\n","            col_ref = F.col(col_name)\n","\n","            if check_type == \"notNull\":\n","                # Fails only if NULL\n","                check_expr = col_ref.isNotNull()\n","            elif check_type == \"notFutureDate\":\n","                # Assumes column is DateType or TimestampType\n","                # Fails if date > today\n","                check_expr = F.when(col_ref.isNull(), True).otherwise(col_ref <= F.current_date())\n","            elif check_type == \"regexMatch\":\n","                pattern = rule.get(\"pattern\")\n","                if pattern:\n","                    # Fails if value is NULL or doesn't match regex\n","                    check_expr = F.when(col_ref.isNull(), False).otherwise(col_ref.rlike(pattern))\n","                else: print(f\"  Skipping rule '{rule_id}': Missing 'pattern' for regexMatch.\")\n","            elif check_type == \"valueInSet\":\n","                allowed_values = rule.get(\"allowed_values\")\n","                if allowed_values is not None and isinstance(allowed_values, list):\n","                     # Fails if value is NULL or not in the set\n","                     check_expr = F.when(col_ref.isNull(), False).otherwise(col_ref.isin(allowed_values))\n","                else: print(f\"  Skipping rule '{rule_id}': Missing/invalid 'allowed_values' list for valueInSet.\")\n","            elif check_type == \"valueInRange\":\n","                min_val = rule.get(\"min_value\")\n","                max_val = rule.get(\"max_value\")\n","                if min_val is not None and max_val is not None:\n","                     # Assumes column is numeric/compatible; fails if NULL or outside range\n","                     # Need to cast literal values to column type potentially, or just let Spark handle comparison\n","                     check_expr = F.when(col_ref.isNull(), True).otherwise(col_ref.between(min_val, max_val))\n","                else: print(f\"  Skipping rule '{rule_id}': Missing 'min_value' or 'max_value' for valueInRange.\")\n","            else:\n","                print(f\"  Skipping rule '{rule_id}': Unknown check_type '{check_type}'.\")\n","                continue # Skip adding this expression\n","\n","            # Add the generated expression to our list\n","            dq_exprs.append(check_expr.alias(dq_col_name))\n","            print(f\"  Added check for rule '{rule_id}' on column '{col_name}'.\")\n","\n","        except Exception as e:\n","            print(f\"  ERROR processing rule '{rule_id}': {e}. Skipping this rule.\")\n","\n","    # Select original columns plus the new DQ check columns\n","    if dq_exprs:\n","        return df.select(F.col(\"*\"), *dq_exprs)\n","    else:\n","        print(\"No applicable and valid DQ checks were generated.\")\n","        return df # Return original df if no rules applied\n","\n","\n","print(\"Dynamic DQ function defined.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"0f594dd2-53ad-41f8-b164-a7990180b401","normalized_state":"finished","queued_time":"2025-04-01T14:18:38.7301558Z","session_start_time":null,"execution_start_time":"2025-04-01T14:18:38.731616Z","execution_finish_time":"2025-04-01T14:18:39.0310999Z","parent_msg_id":"6840bfab-c2be-44fe-8efe-d844972a6d65"},"text/plain":"StatementMeta(, 0f594dd2-53ad-41f8-b164-a7990180b401, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dynamic DQ function defined.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a28b1e91-ff6a-46ed-af89-8f8341511bef"},{"cell_type":"markdown","source":["Demo Point: Walk through the function logic: it takes a DataFrame and rules, iterates rules, checks if the rule's column exists in the DataFrame, builds the appropriate Spark expression based on check_type, and adds the resulting boolean (True=Pass, False=Fail) column to the output DataFrame. Highlight the schema awareness (if col_name not in df_cols)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8fbd1998-393f-44e6-9024-7365a46d4b33"},{"cell_type":"markdown","source":["#### Cell 3: Load Initial Data, Define Schema & Optional Constraints"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be3f2821-2163-4b47-9c2d-e09248901db5"},{"cell_type":"code","source":["# Define expected schema for Batch 1\n","schema_batch1 = StructType([\n","    StructField(\"encounter_id\", StringType(), True),\n","    StructField(\"patient_id\", IntegerType(), True), # Keep as Int for now, handle potential nulls in DQ\n","    StructField(\"provider_id\", IntegerType(), True),\n","    StructField(\"encounter_date\", DateType(), True),\n","    StructField(\"primary_diagnosis_code\", StringType(), True),\n","    StructField(\"visit_type\", StringType(), True),\n","    StructField(\"billable_amount\", DecimalType(10, 2), True) # Use Decimal for currency\n","])\n","\n","# Load Batch 1 with defined schema\n","batch1_df = spark.read.csv(batch1_csv_path, header=True, schema=schema_batch1, dateFormat=\"yyyy-MM-dd\")\n","\n","print(\"Initial Batch 1 Schema & Data:\")\n","batch1_df.printSchema()\n","batch1_df.show(truncate=False)\n","\n","# --- Write initial batch to Delta ---\n","# Drop table first for clean demo reset using SQL\n","spark.sql(f\"DROP TABLE IF EXISTS {encounter_table}\")\n","batch1_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(encounter_table)\n","print(f\"Initial encounter data saved to Delta table: {encounter_table}\")\n","\n","# --- Optional: Add Delta CHECK Constraints for stable rules ---\n","# Example: Enforce patient_id IS NOT NULL at write time\n","# Note: This would cause the write above to fail for row ENC006 if active!\n","# For demo, we might apply dynamic check later instead, or show this failing.\n","# try:\n","#     print(\"\\nAttempting to add CHECK constraint (Patient ID NOT NULL)...\")\n","#     spark.sql(f\"ALTER TABLE {encounter_table} ADD CONSTRAINT patient_id_not_null CHECK (patient_id IS NOT NULL)\")\n","#     print(\"CHECK constraint added successfully.\")\n","# except Exception as e:\n","#     print(f\"Failed to add CHECK constraint (maybe expected if data violates it): {e}\")\n","\n","# Show initial table content\n","print(\"\\nInitial Table Content:\")\n","spark.table(encounter_table).show(truncate=False)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"0f594dd2-53ad-41f8-b164-a7990180b401","normalized_state":"finished","queued_time":"2025-04-01T14:18:43.3146681Z","session_start_time":null,"execution_start_time":"2025-04-01T14:18:43.3158976Z","execution_finish_time":"2025-04-01T14:18:53.4454068Z","parent_msg_id":"6cbbc78f-8466-40ad-934f-95e0ae9bbbed"},"text/plain":"StatementMeta(, 0f594dd2-53ad-41f8-b164-a7990180b401, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Initial Batch 1 Schema & Data:\nroot\n |-- encounter_id: string (nullable = true)\n |-- patient_id: integer (nullable = true)\n |-- provider_id: integer (nullable = true)\n |-- encounter_date: date (nullable = true)\n |-- primary_diagnosis_code: string (nullable = true)\n |-- visit_type: string (nullable = true)\n |-- billable_amount: decimal(10,2) (nullable = true)\n\n+------------+----------+-----------+--------------+----------------------+----------+---------------+\n|encounter_id|patient_id|provider_id|encounter_date|primary_diagnosis_code|visit_type|billable_amount|\n+------------+----------+-----------+--------------+----------------------+----------+---------------+\n|ENC001      |1001      |501        |2025-01-15    |S62.001A              |Office    |150.75         |\n|ENC002      |1002      |502        |2025-01-16    |J45.909               |Telehealth|75.00          |\n|ENC003      |1003      |501        |2025-01-17    |M54.5                 |Office    |165.50         |\n|ENC004      |1001      |501        |2025-01-20    |Z00.00                |Office    |120.00         |\n|ENC005      |1004      |502        |2025-01-21    |R05                   |Emergency |550.25         |\n|ENC006      |NULL      |503        |2025-01-22    |K21.9                 |Inpatient |1200.00        |\n|ENC007      |1002      |502        |2025-10-10    |I10                   |Office    |95.00          |\n+------------+----------+-----------+--------------+----------------------+----------+---------------+\n\nInitial encounter data saved to Delta table: patient_encounters_dynamic\n\nInitial Table Content:\n+------------+----------+-----------+--------------+----------------------+----------+---------------+\n|encounter_id|patient_id|provider_id|encounter_date|primary_diagnosis_code|visit_type|billable_amount|\n+------------+----------+-----------+--------------+----------------------+----------+---------------+\n|ENC001      |1001      |501        |2025-01-15    |S62.001A              |Office    |150.75         |\n|ENC002      |1002      |502        |2025-01-16    |J45.909               |Telehealth|75.00          |\n|ENC003      |1003      |501        |2025-01-17    |M54.5                 |Office    |165.50         |\n|ENC004      |1001      |501        |2025-01-20    |Z00.00                |Office    |120.00         |\n|ENC005      |1004      |502        |2025-01-21    |R05                   |Emergency |550.25         |\n|ENC006      |NULL      |503        |2025-01-22    |K21.9                 |Inpatient |1200.00        |\n|ENC007      |1002      |502        |2025-10-10    |I10                   |Office    |95.00          |\n+------------+----------+-----------+--------------+----------------------+----------+---------------+\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f9dff810-0108-4163-bdd4-76fcfdf0749c"},{"cell_type":"markdown","source":["Demo Point: Explain defining the schema explicitly for robustness. Show writing the first batch. Discuss Delta CHECK constraints as a way to enforce some rules directly on the table (but note it prevents violating data from being written). Show the initial table state."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f48dfb9a-7c8e-4ed8-b777-975b44dbe88f"},{"cell_type":"markdown","source":["#### Cell 4: Load Update Batch and Append with Schema Evolution"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7777de63-7439-4fef-a33c-3d9f4b71a3ef"},{"cell_type":"code","source":["# Define schema for Batch 2 (includes the new column)\n","schema_batch2 = StructType([\n","    StructField(\"encounter_id\", StringType(), True),\n","    StructField(\"patient_id\", IntegerType(), True),\n","    StructField(\"provider_id\", IntegerType(), True),\n","    StructField(\"encounter_date\", DateType(), True),\n","    StructField(\"primary_diagnosis_code\", StringType(), True),\n","    StructField(\"visit_type\", StringType(), True),\n","    StructField(\"billable_amount\", DecimalType(10, 2), True),\n","    StructField(\"quality_metric_score\", IntegerType(), True) # New column\n","])\n","\n","# Load Batch 2\n","batch2_df = spark.read.csv(batch2_csv_path, header=True, schema=schema_batch2, dateFormat=\"yyyy-MM-dd\")\n","\n","print(\"\\nUpdate Batch 2 Schema & Data:\")\n","batch2_df.printSchema()\n","batch2_df.show(truncate=False)\n","\n","# --- Append Batch 2 using Schema Evolution ---\n","print(f\"\\nAppending Batch 2 to Delta table: {encounter_table} with mergeSchema=true...\")\n","try:\n","    batch2_df.write.format(\"delta\") \\\n","        .mode(\"append\") \\\n","        .option(\"mergeSchema\", \"true\") \\\n","        .saveAsTable(encounter_table)\n","    print(\"Append successful with schema evolution.\")\n","except Exception as e:\n","    print(f\"Error appending Batch 2: {e}\")\n","\n","\n","# --- Verify Schema Evolution ---\n","print(\"\\nTable Schema AFTER appending Batch 2:\")\n","evolved_df = spark.table(encounter_table)\n","evolved_df.printSchema()\n","\n","print(\"\\nFull Table Content AFTER appending Batch 2:\")\n","evolved_df.orderBy(\"encounter_date\", \"encounter_id\").show(truncate=False)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"0f594dd2-53ad-41f8-b164-a7990180b401","normalized_state":"finished","queued_time":"2025-04-01T14:18:47.7736536Z","session_start_time":null,"execution_start_time":"2025-04-01T14:18:53.4474133Z","execution_finish_time":"2025-04-01T14:18:58.6027095Z","parent_msg_id":"94a41460-2adc-4341-bff9-3e0d6231f91c"},"text/plain":"StatementMeta(, 0f594dd2-53ad-41f8-b164-a7990180b401, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nUpdate Batch 2 Schema & Data:\nroot\n |-- encounter_id: string (nullable = true)\n |-- patient_id: integer (nullable = true)\n |-- provider_id: integer (nullable = true)\n |-- encounter_date: date (nullable = true)\n |-- primary_diagnosis_code: string (nullable = true)\n |-- visit_type: string (nullable = true)\n |-- billable_amount: decimal(10,2) (nullable = true)\n |-- quality_metric_score: integer (nullable = true)\n\n+------------+----------+-----------+--------------+----------------------+----------+---------------+--------------------+\n|encounter_id|patient_id|provider_id|encounter_date|primary_diagnosis_code|visit_type|billable_amount|quality_metric_score|\n+------------+----------+-----------+--------------+----------------------+----------+---------------+--------------------+\n|ENC008      |1005      |503        |2025-02-01    |F32.9                 |Office    |140.00         |85                  |\n|ENC009      |1001      |501        |2025-02-05    |S62.001A              |Office    |155.50         |92                  |\n|ENC010      |1003      |501        |2025-02-10    |INVALID-CODE          |Office    |130.00         |78                  |\n|ENC011      |1002      |502        |2025-02-12    |J45.909               |Clinic    |80.00          |88                  |\n|ENC012      |1004      |502        |2025-02-15    |R05                   |Emergency |-50.00         |95                  |\n|ENC013      |1005      |503        |2025-02-20    |F32.9                 |Office    |145.00         |105                 |\n|ENC014      |1001      |501        |2025-02-22    |Z00.00                |Office    |125.00         |NULL                |\n+------------+----------+-----------+--------------+----------------------+----------+---------------+--------------------+\n\n\nAppending Batch 2 to Delta table: patient_encounters_dynamic with mergeSchema=true...\nAppend successful with schema evolution.\n\nTable Schema AFTER appending Batch 2:\nroot\n |-- encounter_id: string (nullable = true)\n |-- patient_id: integer (nullable = true)\n |-- provider_id: integer (nullable = true)\n |-- encounter_date: date (nullable = true)\n |-- primary_diagnosis_code: string (nullable = true)\n |-- visit_type: string (nullable = true)\n |-- billable_amount: decimal(10,2) (nullable = true)\n |-- quality_metric_score: integer (nullable = true)\n\n\nFull Table Content AFTER appending Batch 2:\n+------------+----------+-----------+--------------+----------------------+----------+---------------+--------------------+\n|encounter_id|patient_id|provider_id|encounter_date|primary_diagnosis_code|visit_type|billable_amount|quality_metric_score|\n+------------+----------+-----------+--------------+----------------------+----------+---------------+--------------------+\n|ENC001      |1001      |501        |2025-01-15    |S62.001A              |Office    |150.75         |NULL                |\n|ENC002      |1002      |502        |2025-01-16    |J45.909               |Telehealth|75.00          |NULL                |\n|ENC003      |1003      |501        |2025-01-17    |M54.5                 |Office    |165.50         |NULL                |\n|ENC004      |1001      |501        |2025-01-20    |Z00.00                |Office    |120.00         |NULL                |\n|ENC005      |1004      |502        |2025-01-21    |R05                   |Emergency |550.25         |NULL                |\n|ENC006      |NULL      |503        |2025-01-22    |K21.9                 |Inpatient |1200.00        |NULL                |\n|ENC008      |1005      |503        |2025-02-01    |F32.9                 |Office    |140.00         |85                  |\n|ENC009      |1001      |501        |2025-02-05    |S62.001A              |Office    |155.50         |92                  |\n|ENC010      |1003      |501        |2025-02-10    |INVALID-CODE          |Office    |130.00         |78                  |\n|ENC011      |1002      |502        |2025-02-12    |J45.909               |Clinic    |80.00          |88                  |\n|ENC012      |1004      |502        |2025-02-15    |R05                   |Emergency |-50.00         |95                  |\n|ENC013      |1005      |503        |2025-02-20    |F32.9                 |Office    |145.00         |105                 |\n|ENC014      |1001      |501        |2025-02-22    |Z00.00                |Office    |125.00         |NULL                |\n|ENC007      |1002      |502        |2025-10-10    |I10                   |Office    |95.00          |NULL                |\n+------------+----------+-----------+--------------+----------------------+----------+---------------+--------------------+\n\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8ce540eb-e476-4c17-b8e4-d7ffecbfc32d"},{"cell_type":"markdown","source":["Demo Point: Show loading the second batch with the new column. Explain the critical mode(\"append\").option(\"mergeSchema\", \"true\") flags. Show the table schema after the append – highlight that quality_metric_score has been automatically added. Display the full table containing data from both batches."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b800810c-fa2b-4ae4-98e0-80a6d2cfa1e9"},{"cell_type":"markdown","source":["#### Cell 5: Load Dynamic DQ Rules"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"697b9a5f-ef72-4277-a3ad-5205c773febb"},{"cell_type":"code","source":["import json \n","\n","# Define the relative path within the Lakehouse Files section\n","# This path should work with spark.read relative to the root of the default Lakehouse\n","rules_json_path = \"Files/landing/healthcare/dq_rules.json\"\n","\n","# Load DQ rules from JSON file using Spark's text reader\n","print(f\"Attempting to load DQ rules from: {rules_json_path}\")\n","rules_list = [] # Initialize empty list\n","\n","try:\n","    # Read the entire file content into a single string using Spark\n","    # wholetext=True is useful if the JSON is multi-line formatted, otherwise default works too\n","    rules_file_content = spark.read.text(rules_json_path, wholetext=True).first()[0]\n","\n","    # Parse the JSON string loaded by Spark into a Python list/object\n","    rules_list = json.loads(rules_file_content)\n","\n","    print(\"Successfully loaded DQ rules using spark.read.text:\")\n","    print(json.dumps(rules_list, indent=2))\n","\n","except Exception as e:\n","    print(f\"Error loading DQ rules from {rules_json_path} using Spark:\")\n","    print(e)\n","    # Add specific checks if needed\n","    if \"Path does not exist\" in str(e):\n","         print(f\"\\n--- Troubleshooting ---\")\n","         print(f\"1. Verify the file exists at '{rules_json_path}' in the Lakehouse Explorer UI.\")\n","         print(f\"2. Ensure the correct Lakehouse is attached as default to this notebook.\")\n","         print(f\"3. As a fallback, try using the full ABFSS path (find IDs in Lakehouse/Workspace settings):\")\n","         print(f\"   Example: full_path = 'abfss://YOUR_WORKSPACE_ID@onelake.dfs.fabric.microsoft.com/YOUR_LAKEHOUSE_ID/Files/landing/healthcare/dq_rules.json'\")\n","         # full_path = 'abfss://...' # Construct actual path if needed\n","         # rules_file_content = spark.read.text(full_path, wholetext=True).first()[0]\n","         # rules_list = json.loads(rules_file_content)\n","\n","    rules_list = [] # Ensure rules_list is empty on error\n","\n","# Verify rules_list content (will be empty if loading failed)\n","if not rules_list:\n","    print(\"\\nWARNING: DQ rules list is empty. Subsequent DQ checks will be skipped.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"0f594dd2-53ad-41f8-b164-a7990180b401","normalized_state":"finished","queued_time":"2025-04-01T14:18:58.8129116Z","session_start_time":null,"execution_start_time":"2025-04-01T14:18:58.8142767Z","execution_finish_time":"2025-04-01T14:18:59.74068Z","parent_msg_id":"3f4a9c10-82ae-4ffc-a32a-61dbb5bea28a"},"text/plain":"StatementMeta(, 0f594dd2-53ad-41f8-b164-a7990180b401, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Attempting to load DQ rules from: Files/landing/healthcare/dq_rules.json\nSuccessfully loaded DQ rules using spark.read.text:\n[\n  {\n    \"rule_id\": \"DQ001\",\n    \"column_name\": \"patient_id\",\n    \"check_type\": \"notNull\",\n    \"is_active\": true,\n    \"description\": \"Patient ID must not be null\"\n  },\n  {\n    \"rule_id\": \"DQ002\",\n    \"column_name\": \"encounter_date\",\n    \"check_type\": \"notFutureDate\",\n    \"is_active\": true,\n    \"description\": \"Encounter date cannot be in the future\"\n  },\n  {\n    \"rule_id\": \"DQ003\",\n    \"column_name\": \"primary_diagnosis_code\",\n    \"check_type\": \"regexMatch\",\n    \"pattern\": \"^[A-Z][0-9]{2}(\\\\.[A-Z0-9]{1,4})?$\",\n    \"is_active\": true,\n    \"description\": \"Primary Diagnosis Code must be valid ICD-10 format (approx.)\"\n  },\n  {\n    \"rule_id\": \"DQ004\",\n    \"column_name\": \"visit_type\",\n    \"check_type\": \"valueInSet\",\n    \"allowed_values\": [\n      \"Office\",\n      \"Telehealth\",\n      \"Inpatient\",\n      \"Emergency\"\n    ],\n    \"is_active\": true,\n    \"description\": \"Visit Type must be one of the allowed values\"\n  },\n  {\n    \"rule_id\": \"DQ005\",\n    \"column_name\": \"billable_amount\",\n    \"check_type\": \"valueInRange\",\n    \"min_value\": 0.0,\n    \"max_value\": 100000.0,\n    \"is_active\": true,\n    \"description\": \"Billable amount must be between 0 and 100,000\"\n  },\n  {\n    \"rule_id\": \"DQ006\",\n    \"column_name\": \"quality_metric_score\",\n    \"check_type\": \"valueInRange\",\n    \"min_value\": 0,\n    \"max_value\": 100,\n    \"is_active\": true,\n    \"description\": \"Quality score must be between 0 and 100 (if present)\"\n  }\n]\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e57a4ca6-1c39-4fb6-9b63-3fab3dfdbf74"},{"cell_type":"markdown","source":["Demo Point: Show the external definition of DQ rules. Explain how this allows modifying rules without changing the core Spark code. (Note: Reading directly from Lakehouse Files might require using the full ABFSS path or Spark's read capabilities; the code above simulates loading for ease of demo setup)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"56172adc-7ea0-4560-b5a7-929245d7ee2d"},{"cell_type":"markdown","source":["#### Cell 6: Apply Dynamic DQ Checks to Evolved Table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"13e27aa9-64fd-4934-b207-cd35849fbf2b"},{"cell_type":"code","source":["# Read the full evolved table\n","full_encounters_df = spark.table(encounter_table)\n","\n","# Apply the dynamic DQ function\n","if rules_list: # Check if rules were loaded\n","    dq_results_df = apply_dynamic_dq(full_encounters_df, rules_list)\n","\n","    print(\"\\nSchema with DQ Check Columns:\")\n","    dq_results_df.printSchema()\n","\n","    print(\"\\nSample Data with DQ Results (True = Pass):\")\n","    # Select some key columns and the DQ columns to show results\n","    cols_to_show = [\"encounter_id\", \"patient_id\", \"encounter_date\", \"primary_diagnosis_code\", \"visit_type\", \"billable_amount\", \"quality_metric_score\"] \\\n","                   + [f\"DQ_{rule['rule_id']}\" for rule in rules_list if f\"DQ_{rule['rule_id']}\" in dq_results_df.columns]\n","    dq_results_df.select(cols_to_show).orderBy(\"encounter_date\", \"encounter_id\").show(truncate=False)\n","else:\n","    print(\"Skipping DQ checks as rules were not loaded.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"0f594dd2-53ad-41f8-b164-a7990180b401","normalized_state":"finished","queued_time":"2025-04-01T14:19:03.9983956Z","session_start_time":null,"execution_start_time":"2025-04-01T14:19:03.9996243Z","execution_finish_time":"2025-04-01T14:19:05.5018761Z","parent_msg_id":"68575544-f8af-49cf-850a-d22c24c85f5a"},"text/plain":"StatementMeta(, 0f594dd2-53ad-41f8-b164-a7990180b401, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Applying 6 DQ rules dynamically...\n  Added check for rule 'DQ001' on column 'patient_id'.\n  Added check for rule 'DQ002' on column 'encounter_date'.\n  Added check for rule 'DQ003' on column 'primary_diagnosis_code'.\n  Added check for rule 'DQ004' on column 'visit_type'.\n  Added check for rule 'DQ005' on column 'billable_amount'.\n  Added check for rule 'DQ006' on column 'quality_metric_score'.\n\nSchema with DQ Check Columns:\nroot\n |-- encounter_id: string (nullable = true)\n |-- patient_id: integer (nullable = true)\n |-- provider_id: integer (nullable = true)\n |-- encounter_date: date (nullable = true)\n |-- primary_diagnosis_code: string (nullable = true)\n |-- visit_type: string (nullable = true)\n |-- billable_amount: decimal(10,2) (nullable = true)\n |-- quality_metric_score: integer (nullable = true)\n |-- DQ_DQ001: boolean (nullable = false)\n |-- DQ_DQ002: boolean (nullable = true)\n |-- DQ_DQ003: boolean (nullable = true)\n |-- DQ_DQ004: boolean (nullable = true)\n |-- DQ_DQ005: boolean (nullable = true)\n |-- DQ_DQ006: boolean (nullable = true)\n\n\nSample Data with DQ Results (True = Pass):\n+------------+----------+--------------+----------------------+----------+---------------+--------------------+--------+--------+--------+--------+--------+--------+\n|encounter_id|patient_id|encounter_date|primary_diagnosis_code|visit_type|billable_amount|quality_metric_score|DQ_DQ001|DQ_DQ002|DQ_DQ003|DQ_DQ004|DQ_DQ005|DQ_DQ006|\n+------------+----------+--------------+----------------------+----------+---------------+--------------------+--------+--------+--------+--------+--------+--------+\n|ENC001      |1001      |2025-01-15    |S62.001A              |Office    |150.75         |NULL                |true    |true    |true    |true    |true    |true    |\n|ENC002      |1002      |2025-01-16    |J45.909               |Telehealth|75.00          |NULL                |true    |true    |true    |true    |true    |true    |\n|ENC003      |1003      |2025-01-17    |M54.5                 |Office    |165.50         |NULL                |true    |true    |true    |true    |true    |true    |\n|ENC004      |1001      |2025-01-20    |Z00.00                |Office    |120.00         |NULL                |true    |true    |true    |true    |true    |true    |\n|ENC005      |1004      |2025-01-21    |R05                   |Emergency |550.25         |NULL                |true    |true    |true    |true    |true    |true    |\n|ENC006      |NULL      |2025-01-22    |K21.9                 |Inpatient |1200.00        |NULL                |false   |true    |true    |true    |true    |true    |\n|ENC008      |1005      |2025-02-01    |F32.9                 |Office    |140.00         |85                  |true    |true    |true    |true    |true    |true    |\n|ENC009      |1001      |2025-02-05    |S62.001A              |Office    |155.50         |92                  |true    |true    |true    |true    |true    |true    |\n|ENC010      |1003      |2025-02-10    |INVALID-CODE          |Office    |130.00         |78                  |true    |true    |false   |true    |true    |true    |\n|ENC011      |1002      |2025-02-12    |J45.909               |Clinic    |80.00          |88                  |true    |true    |true    |false   |true    |true    |\n|ENC012      |1004      |2025-02-15    |R05                   |Emergency |-50.00         |95                  |true    |true    |true    |true    |false   |true    |\n|ENC013      |1005      |2025-02-20    |F32.9                 |Office    |145.00         |105                 |true    |true    |true    |true    |true    |false   |\n|ENC014      |1001      |2025-02-22    |Z00.00                |Office    |125.00         |NULL                |true    |true    |true    |true    |true    |true    |\n|ENC007      |1002      |2025-10-10    |I10                   |Office    |95.00          |NULL                |true    |false   |true    |true    |true    |true    |\n+------------+----------+--------------+----------------------+----------+---------------+--------------------+--------+--------+--------+--------+--------+--------+\n\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"503526db-54b0-45fa-a819-106a09df011e"},{"cell_type":"markdown","source":["Demo Point: Show calling the apply_dynamic_dq function. Point out the output DataFrame now includes new boolean columns (DQ_DQ001, DQ_DQ002, etc.). Highlight specific rows from the sample data and show how they passed/failed certain checks (e.g., ENC006 fails DQ001, ENC007 fails DQ002, ENC010 fails DQ003, etc.). Show that DQ006 was applied only after the schema evolved."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"195cc4aa-86bc-4ae4-946b-71f142040dde"},{"cell_type":"markdown","source":["#### Cell 7: Analyze DQ Results and Save Failures"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"288b31e1-932b-4d65-b2e1-48709727dc75"},{"cell_type":"code","source":["# Check if the DataFrame from the previous step exists and is not None\n","if 'dq_results_df' in locals() and dq_results_df is not None:\n","    print(\"\\nAnalyzing DQ results...\")\n","    # --- Identify failed records and the rules they failed (using PySpark functions) ---\n","\n","    # Get the names of the dynamically added DQ check columns (e.g., 'DQ_DQ001', 'DQ_DQ002')\n","    dq_check_cols = [c for c in dq_results_df.columns if c.startswith(\"DQ_\")]\n","\n","    # Create an array of rule IDs for checks that failed (where the DQ column is False)\n","    failed_rules_cols_exprs = []\n","    if dq_check_cols: # Proceed only if DQ columns were actually generated\n","        print(f\"  Generating failed rules list based on columns: {dq_check_cols}\")\n","        for c in dq_check_cols:\n","            # Extract the Rule ID part (e.g., 'DQ001' from 'DQ_DQ001')\n","            rule_id = c[3:]\n","            # Use when().otherwise() to create a column expression:\n","            # It yields the rule_id string if the check column 'c' is False, otherwise it yields NULL\n","            failed_rules_cols_exprs.append(F.when(F.col(c) == False, F.lit(rule_id)).otherwise(F.lit(None)))\n","\n","        # Create the final array column using F.array() to combine the conditional expressions,\n","        # and then use the higher-order function F.filter() to remove the NULLs (passed checks)\n","        failed_rules_array_col = F.filter(F.array(*failed_rules_cols_exprs), lambda x: x.isNotNull())\n","    else:\n","        # If no DQ columns were found/generated, create an empty array literal for consistency\n","        print(\"  No DQ check columns found to analyze.\")\n","        failed_rules_array_col = F.array().cast(\"array<string>\")\n","\n","\n","    # Add the 'failed_rules' array and the overall 'dq_status' column\n","    # Apply these to the dq_results_df which contains the original data + boolean DQ check columns\n","    dq_summary_df = dq_results_df.withColumn(\"failed_rules\", failed_rules_array_col) \\\n","                               .withColumn(\"dq_status\", F.when(F.size(F.col(\"failed_rules\")) == 0, \"PASS\").otherwise(\"FAIL\"))\n","\n","    print(\"\\nDQ Summary (Status and Failed Rules):\")\n","    # Show status and the array of failed rule IDs for all records\n","    dq_summary_df.select(\"encounter_id\", \"patient_id\", \"dq_status\", \"failed_rules\").orderBy(\"encounter_date\", \"encounter_id\").show(truncate=False)\n","\n","    # --- Filter for only failed records ---\n","    failed_records_df = dq_summary_df.filter(F.col(\"dq_status\") == \"FAIL\")\n","    failed_count = failed_records_df.count() # Cache count might be useful if reused\n","\n","    print(f\"\\nIdentified {failed_count} records failing DQ checks.\")\n","\n","    if failed_count > 0:\n","        print(\"Failed Records Details (showing status and failed rules):\")\n","        # Show failed records with their failed rules\n","        failed_records_df.select(\"encounter_id\", \"patient_id\", \"dq_status\", \"failed_rules\").orderBy(\"encounter_date\", \"encounter_id\").show(truncate=False)\n","\n","        # --- Save failed records details to another Delta table ---\n","        # Select original data columns + status + failed rules array for the output table.\n","        # This avoids saving the intermediate boolean DQ check columns (DQ_DQ001, etc.).\n","        # We need the original columns from the DataFrame before DQ checks were added.\n","        # Assuming 'full_encounters_df' from Cell 6 holds that state.\n","        if 'full_encounters_df' in locals():\n","             original_data_cols = full_encounters_df.columns\n","             # Define columns for the final output table\n","             output_cols = original_data_cols + [\"dq_status\", \"failed_rules\"]\n","             # Select these columns from the already filtered failed_records_df\n","             df_to_save = failed_records_df.select(output_cols)\n","\n","             try:\n","                # Write to the results table, overwriting previous results for the demo\n","                df_to_save.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(dq_results_table)\n","                print(f\"\\nFailed records saved to Delta table: {dq_results_table}\")\n","             except Exception as e:\n","                print(f\"\\nError saving DQ results: {e}\")\n","        else:\n","             print(\"\\nCould not find 'full_encounters_df' to determine original columns. Skipping save.\")\n","    else:\n","        print(\"\\nNo failed records to save.\")\n","        # Optional: Drop the results table if it exists and there are no failures, for a clean state\n","        # spark.sql(f\"DROP TABLE IF EXISTS {dq_results_table}\")\n","        # print(f\"Ensured results table {dq_results_table} is clear as no failures were found.\")\n","\n","else:\n","    print(\"DQ results DataFrame ('dq_results_df') not found or is None. Skipping analysis and save.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"0f594dd2-53ad-41f8-b164-a7990180b401","normalized_state":"finished","queued_time":"2025-04-01T14:19:08.8011261Z","session_start_time":null,"execution_start_time":"2025-04-01T14:19:08.8025039Z","execution_finish_time":"2025-04-01T14:19:14.9873416Z","parent_msg_id":"5929e38a-8564-46d0-a31c-4525424ff75b"},"text/plain":"StatementMeta(, 0f594dd2-53ad-41f8-b164-a7990180b401, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nAnalyzing DQ results...\n  Generating failed rules list based on columns: ['DQ_DQ001', 'DQ_DQ002', 'DQ_DQ003', 'DQ_DQ004', 'DQ_DQ005', 'DQ_DQ006']\n\nDQ Summary (Status and Failed Rules):\n+------------+----------+---------+------------+\n|encounter_id|patient_id|dq_status|failed_rules|\n+------------+----------+---------+------------+\n|ENC001      |1001      |PASS     |[]          |\n|ENC002      |1002      |PASS     |[]          |\n|ENC003      |1003      |PASS     |[]          |\n|ENC004      |1001      |PASS     |[]          |\n|ENC005      |1004      |PASS     |[]          |\n|ENC006      |NULL      |FAIL     |[DQ001]     |\n|ENC008      |1005      |PASS     |[]          |\n|ENC009      |1001      |PASS     |[]          |\n|ENC010      |1003      |FAIL     |[DQ003]     |\n|ENC011      |1002      |FAIL     |[DQ004]     |\n|ENC012      |1004      |FAIL     |[DQ005]     |\n|ENC013      |1005      |FAIL     |[DQ006]     |\n|ENC014      |1001      |PASS     |[]          |\n|ENC007      |1002      |FAIL     |[DQ002]     |\n+------------+----------+---------+------------+\n\n\nIdentified 6 records failing DQ checks.\nFailed Records Details (showing status and failed rules):\n+------------+----------+---------+------------+\n|encounter_id|patient_id|dq_status|failed_rules|\n+------------+----------+---------+------------+\n|ENC006      |NULL      |FAIL     |[DQ001]     |\n|ENC010      |1003      |FAIL     |[DQ003]     |\n|ENC011      |1002      |FAIL     |[DQ004]     |\n|ENC012      |1004      |FAIL     |[DQ005]     |\n|ENC013      |1005      |FAIL     |[DQ006]     |\n|ENC007      |1002      |FAIL     |[DQ002]     |\n+------------+----------+---------+------------+\n\n\nFailed records saved to Delta table: patient_encounters_dq_results\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"998d806d-3932-415c-be99-7f1d67488ae4"},{"cell_type":"markdown","source":["Demo Point: Show how to analyze the boolean DQ columns to create a summary status (PASS/FAIL) and a list of exactly which rules failed for each record. Filter to isolate the failing records and save them for review/correction."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ab4d08e7-30ad-44c7-bd10-05990fdf284d"},{"cell_type":"markdown","source":["#### Cell 8: Query DQ Results & Discussion"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d7f8a5f5-1a81-4833-b5e9-7393f011a3e4"},{"cell_type":"code","source":["%%sql\n","-- Query the DQ results table (showing only failed records)\n","SELECT\n","    encounter_id,\n","    patient_id,\n","    encounter_date,\n","    primary_diagnosis_code,\n","    visit_type,\n","    billable_amount,\n","    quality_metric_score,\n","    dq_status,\n","    failed_rules -- Show array of failed rule IDs\n","FROM\n","    patient_encounters_dq_results -- Use table name directly\n","ORDER BY\n","    encounter_date, encounter_id;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"0f594dd2-53ad-41f8-b164-a7990180b401","normalized_state":"finished","queued_time":"2025-04-01T14:19:16.2094415Z","session_start_time":null,"execution_start_time":"2025-04-01T14:19:16.2106105Z","execution_finish_time":"2025-04-01T14:19:19.615997Z","parent_msg_id":"9db4db5a-cbd9-406c-a7fd-85a7c6927e77"},"text/plain":"StatementMeta(, 0f594dd2-53ad-41f8-b164-a7990180b401, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":9,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"encounter_id","type":"string","nullable":true,"metadata":{}},{"name":"patient_id","type":"integer","nullable":true,"metadata":{}},{"name":"encounter_date","type":"date","nullable":true,"metadata":{}},{"name":"primary_diagnosis_code","type":"string","nullable":true,"metadata":{}},{"name":"visit_type","type":"string","nullable":true,"metadata":{}},{"name":"billable_amount","type":"decimal(10,2)","nullable":true,"metadata":{}},{"name":"quality_metric_score","type":"integer","nullable":true,"metadata":{}},{"name":"dq_status","type":"string","nullable":true,"metadata":{}},{"name":"failed_rules","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},"data":[["ENC006",null,"2025-01-22","K21.9","Inpatient","1200.00",null,"FAIL",["DQ001"]],["ENC010",1003,"2025-02-10","INVALID-CODE","Office","130.00",78,"FAIL",["DQ003"]],["ENC011",1002,"2025-02-12","J45.909","Clinic","80.00",88,"FAIL",["DQ004"]],["ENC012",1004,"2025-02-15","R05","Emergency","-50.00",95,"FAIL",["DQ005"]],["ENC013",1005,"2025-02-20","F32.9","Office","145.00",105,"FAIL",["DQ006"]],["ENC007",1002,"2025-10-10","I10","Office","95.00",null,"FAIL",["DQ002"]]]},"text/plain":"<Spark SQL result set with 6 rows and 9 fields>"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"89b29ae8-ad91-4d52-9133-94ed86050e52"},{"cell_type":"markdown","source":["Demo Point: Query the table containing only the failed records. Discuss the benefits: Schema evolution handled new data columns automatically. DQ rules were applied dynamically based on external definitions and the current schema. Specific failures are identified for remediation. This approach is flexible and maintainable."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e08c28d-b521-4057-9af9-3e0355900f2a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"1d3b6cad-9230-4783-9df1-f821711b1c8f","default_lakehouse_name":"HealthcareData","default_lakehouse_workspace_id":"8ecba42e-a6c5-4672-854f-d570b4f45d10"}}},"nbformat":4,"nbformat_minor":5}