{"cells":[{"cell_type":"markdown","source":[" ### Efficiently Processing & Querying Highly Nested, Schema-Variable JSON\n","\n","* **Problem:** Source systems often output deeply nested JSON with arrays, complex objects, and fields that might appear/disappear between records. Flattening everything can be inefficient or lead to massive tables. Querying specific nested elements directly can be cumbersome with basic tools.\n","* **Trickiness:** Schema evolution, deeply nested structures, arrays, inconsistent fields make simple flattening or SQL querying difficult and inefficient. `explode()` can cause row explosion.\n","* **Technique (Spark Notebook):**\n","    * **Selective Flattening & Struct Navigation:** Instead of full flattening, use PySpark to read the JSON (inferring schema or providing a robust one). Employ `select()` with complex path expressions (`col(\"address.city\")`, `col(\"orders[0].item_id\")`) and functions like `explode()` (for arrays), `element_at()` (for specific array elements or map values), and `getField()` to extract *only* the necessary top-level and specific nested fields into a structured DataFrame. Keep other less-used nested parts as `StructType` columns.\n","* **Demo Focus:** Read complex `JSON` from `OneLake`/`Lakehouse` -> Apply selective flattening/struct navigation -> Show querying the resulting DataFrame with a mix of flat columns and accessible structs -> Optionally show a `Pandas UDF` tackling a particularly messy part -> Write results to a `Delta Lake` table, queryable via the `SQL endpoint`.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ae0a392d-0f30-4f1a-b579-e4c383732c44"},{"cell_type":"markdown","source":["#### Reset Demo"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2f0a1ef3-1a7a-48ac-8011-a1c9aafe1c5b"},{"cell_type":"markdown","source":["Trying this configure command to improve Default Lakehouse stability. Occasionally getting 400 errors on reading the JSON file using the relative path. See here: https://community.fabric.microsoft.com/t5/Data-Engineering/400-error-when-accessing-lakehouse-files-from-notebook/m-p/4315410"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d5a03e64-95c9-48c4-84d6-e28f833c1e81"},{"cell_type":"code","source":["%%configure\n","{\n","    \"defaultLakehouse\": {\n","        \"name\": \"HealthcareData\"\n","}}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"finished","livy_statement_state":"available","session_id":"3aab02fa-84d7-4888-8d7d-a9c7f6d144b9","normalized_state":"finished","queued_time":"2025-04-01T21:49:18.0154713Z","session_start_time":"2025-04-01T21:49:18.2272736Z","execution_start_time":"2025-04-01T21:49:22.353117Z","execution_finish_time":"2025-04-01T21:49:22.468424Z","parent_msg_id":"3f0da00c-db3b-4002-a218-a845756fde1a"},"text/plain":"StatementMeta(, 3aab02fa-84d7-4888-8d7d-a9c7f6d144b9, -1, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a66342ae-cd08-4111-a234-09f048017184"},{"cell_type":"code","source":["%%sql\n","\n","DROP TABLE IF EXISTS orders_processed"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":2,"statement_ids":[2],"state":"finished","livy_statement_state":"available","session_id":"3aab02fa-84d7-4888-8d7d-a9c7f6d144b9","normalized_state":"finished","queued_time":"2025-04-01T21:49:18.0170324Z","session_start_time":null,"execution_start_time":"2025-04-01T21:49:23.8357862Z","execution_finish_time":"2025-04-01T21:49:37.8916119Z","parent_msg_id":"b7c0fa3b-bd7b-45bc-9b39-4d8fed75ddc2"},"text/plain":"StatementMeta(, 3aab02fa-84d7-4888-8d7d-a9c7f6d144b9, 2, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":2,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"33735e3e-9898-46e4-83f7-5af329f9c1cb"},{"cell_type":"markdown","source":["#### Cell 1: Setup & Imports"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"65b6392e-5c39-4291-a52f-ff5f2440f498"},{"cell_type":"code","source":["# Required import for the Fabric Warehouse/Lakehouse Spark Connector\n","import com.microsoft.spark.fabric\n","\n","# Import PySpark functions\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import * # Optional, for defining schema manually if needed\n","\n","print(\"Setup complete. PySpark functions imported.\")\n","\n","# Define file paths (Adjust these to your Lakehouse structure)\n","# Assuming 'Files/landing/orders/' directory in your Lakehouse\n","json_file_path = \"Files/landing/orders/orders.jsonl\"\n","# Define output delta table path (relative to 'Tables' folder)\n","output_delta_table_name = \"orders_processed\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"3aab02fa-84d7-4888-8d7d-a9c7f6d144b9","normalized_state":"finished","queued_time":"2025-04-01T21:49:18.0186365Z","session_start_time":null,"execution_start_time":"2025-04-01T21:49:42.1332039Z","execution_finish_time":"2025-04-01T21:49:42.8311525Z","parent_msg_id":"a2bf88a0-01fe-4f73-a3ec-be2c88eb47f1"},"text/plain":"StatementMeta(, 3aab02fa-84d7-4888-8d7d-a9c7f6d144b9, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Setup complete. PySpark functions imported.\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"80b17290-4851-4457-aa31-2a2a4f650ae0"},{"cell_type":"markdown","source":["###### Highlights of this data:\n","\n","    Nesting: customer, address, items, shipping_details.\n","    Arrays: tags, items.\n","    Optional Fields: middle_name (record 2), discount in items (record 3), tracking_no missing in shipping_details (record 4).\n","    Null Objects/Empty Arrays: shipping_details (record 2), tags (record 2), items (record 4).\n","    Mixed Types (Potentially): customer_id is numeric (123, 456) and string (\"CUST-789\"). Spark might infer string for the column."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd8f4c14-f355-443f-a002-18827cfd1abb"},{"cell_type":"markdown","source":["##### Cell 2: Read JSON and Inspect Schema"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc4e0209-4174-4a05-82d2-0b59bc0455be"},{"cell_type":"code","source":["# Read the JSON lines file, letting Spark infer the schema\n","try:\n","    raw_df = spark.read.json(json_file_path)\n","    print(\"Successfully read JSON file.\")\n","    print(\"Inferred Schema:\")\n","    raw_df.printSchema()\n","    print(\"\\nSample Data (Raw):\")\n","    raw_df.show(truncate=False) # Show raw structure\n","except Exception as e:\n","    print(f\"Error reading JSON: {e}\")\n","    # Consider stopping the notebook if read fails\n","    mssparkutils.notebook.exit(\"Failed to read source JSON\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"3aab02fa-84d7-4888-8d7d-a9c7f6d144b9","normalized_state":"finished","queued_time":"2025-04-01T21:49:18.0201792Z","session_start_time":null,"execution_start_time":"2025-04-01T21:49:42.8333119Z","execution_finish_time":"2025-04-01T21:49:44.323055Z","parent_msg_id":"0035c506-a905-4a90-9901-c16500506f83"},"text/plain":"StatementMeta(, 3aab02fa-84d7-4888-8d7d-a9c7f6d144b9, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully read JSON file.\nInferred Schema:\nroot\n |-- customer: struct (nullable = true)\n |    |-- address: struct (nullable = true)\n |    |    |-- city: string (nullable = true)\n |    |    |-- state: string (nullable = true)\n |    |    |-- street: string (nullable = true)\n |    |    |-- zip: string (nullable = true)\n |    |-- customer_id: string (nullable = true)\n |    |-- email: string (nullable = true)\n |    |-- first_name: string (nullable = true)\n |    |-- last_name: string (nullable = true)\n |    |-- middle_name: string (nullable = true)\n |    |-- tags: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- is_pickup: boolean (nullable = true)\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- discount: double (nullable = true)\n |    |    |-- item_id: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- price: double (nullable = true)\n |    |    |-- quantity: long (nullable = true)\n |-- order_id: string (nullable = true)\n |-- order_timestamp: string (nullable = true)\n |-- shipping_details: struct (nullable = true)\n |    |-- method: string (nullable = true)\n |    |-- tracking_no: string (nullable = true)\n |-- total_amount: double (nullable = true)\n\n\nSample Data (Raw):\n+----------------------------------------------------------------------------------------------------------+---------+----------------------------------------------------------------------+--------+--------------------+----------------------+------------+\n|customer                                                                                                  |is_pickup|items                                                                 |order_id|order_timestamp     |shipping_details      |total_amount|\n+----------------------------------------------------------------------------------------------------------+---------+----------------------------------------------------------------------+--------+--------------------+----------------------+------------+\n|{{Anytown, CA, 123 Main St, 90210}, 123, alice.s@example.com, Alice, Smith, NULL, [vip, newsletter]}      |false    |[{NULL, P-ABC, Product A, 50.0, 1}, {NULL, P-XYZ, Product X, 15.5, 2}]|ORD-1001|2025-03-28T10:05:00Z|{Express, T123456789} |81.0        |\n|{{Otherville, NY, 456 Oak Ave, 10001}, 456, bobj@example.net, Bob, Jones, J., []}                         |true     |[{NULL, P-LMN, Product L, 120.0, 1}]                                  |ORD-1002|2025-03-28T11:15:30Z|NULL                  |120.0       |\n|{{Anytown, CA, 789 Pine Rd, 90211}, CUST-789, charlie.d@example.org, Charlie, Davis, NULL, [new_customer]}|false    |[{5.0, P-ABC, Product A, 50.0, 1}, {NULL, P-QRS, Product Q, 10.0, 3}] |ORD-1003|2025-03-28T12:45:10Z|{Standard, T987654321}|75.0        |\n|{{Anytown, CA, 123 Main St, 90210}, 123, alice.s@example.com, Alice, Smith, NULL, [vip, newsletter]}      |false    |[]                                                                    |ORD-1004|2025-03-28T13:20:00Z|{Standard, NULL}      |0.0         |\n+----------------------------------------------------------------------------------------------------------+---------+----------------------------------------------------------------------+--------+--------------------+----------------------+------------+\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"02a91778-b43e-45a5-92ec-7aa8ef80f821"},{"cell_type":"markdown","source":["###### Discussion Point: \n","\n","Discuss the inferred schema. Point out the struct types for customer, address, items (as array<struct>), shipping_details. Note the data types inferred (e.g., for customer_id). Show how complex it looks raw."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b319a2dc-4bd4-41dc-bba7-2ed1f88f8c29"},{"cell_type":"markdown","source":["#### Cell 3: Selective Flattening & Struct Navigation"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"811d7aa1-4fa0-45e5-906c-365b3d454a5d"},{"cell_type":"code","source":["# Select top-level fields and specific nested fields\n","# Explode the items array to get one row per item per order\n","# Keep the full address as a struct for demonstration\n","\n","if 'raw_df' in locals(): # Check if previous cell ran successfully\n","    processed_df = raw_df.select(\n","        F.col(\"order_id\"),\n","        F.col(\"order_timestamp\"),\n","        # Select specific customer fields, renaming for clarity\n","        F.col(\"customer.customer_id\").alias(\"customer_id\"), # Note potential type mixing\n","        F.col(\"customer.email\").alias(\"customer_email\"),\n","        F.col(\"customer.first_name\").alias(\"customer_fname\"),\n","        F.col(\"customer.last_name\").alias(\"customer_lname\"),\n","        # Handle optional middle_name - will be null if missing\n","        F.col(\"customer.middle_name\").alias(\"customer_mname\"),\n","        # Select specific address fields\n","        F.col(\"customer.address.city\").alias(\"customer_city\"),\n","        F.col(\"customer.address.state\").alias(\"customer_state\"),\n","        # Keep the full address struct as well\n","        F.col(\"customer.address\").alias(\"customer_address_struct\"),\n","        # Handle potentially null shipping details before accessing nested fields\n","        F.col(\"shipping_details.method\").alias(\"shipping_method\"),\n","        F.col(\"shipping_details.tracking_no\").alias(\"shipping_tracking\"), # Will be null if missing\n","        # Explode the items array - this creates multiple rows per order if >1 item\n","        # Use posexplode if you need the position in the array\n","        F.explode_outer(\"items\").alias(\"item\") # explode_outer handles null/empty arrays gracefully\n","    ).select(\n","        \"*\", # Select all columns generated so far\n","        # Now select fields from the exploded 'item' struct\n","        F.col(\"item.item_id\").alias(\"item_id\"),\n","        F.col(\"item.name\").alias(\"item_name\"),\n","        F.col(\"item.quantity\").alias(\"item_quantity\"),\n","        F.col(\"item.price\").alias(\"item_price\"),\n","        # Handle optional discount, defaulting to 0.0 if null or missing\n","        F.coalesce(F.col(\"item.discount\"), F.lit(0.0)).alias(\"item_discount\")\n","    ).drop(\"item\") # Drop the intermediate exploded struct column\n","\n","    print(\"Schema after selection and explosion:\")\n","    processed_df.printSchema()\n","    print(\"\\nSample Data (Processed):\")\n","    processed_df.show(truncate=False)\n","else:\n","    print(\"raw_df not found. Skipping processing.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"3aab02fa-84d7-4888-8d7d-a9c7f6d144b9","normalized_state":"finished","queued_time":"2025-04-01T21:49:18.0216296Z","session_start_time":null,"execution_start_time":"2025-04-01T21:49:44.325246Z","execution_finish_time":"2025-04-01T21:49:45.1315556Z","parent_msg_id":"7d928c0f-a994-41ce-b71e-4b74f0dfa1c1"},"text/plain":"StatementMeta(, 3aab02fa-84d7-4888-8d7d-a9c7f6d144b9, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Schema after selection and explosion:\nroot\n |-- order_id: string (nullable = true)\n |-- order_timestamp: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- customer_email: string (nullable = true)\n |-- customer_fname: string (nullable = true)\n |-- customer_lname: string (nullable = true)\n |-- customer_mname: string (nullable = true)\n |-- customer_city: string (nullable = true)\n |-- customer_state: string (nullable = true)\n |-- customer_address_struct: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |    |-- street: string (nullable = true)\n |    |-- zip: string (nullable = true)\n |-- shipping_method: string (nullable = true)\n |-- shipping_tracking: string (nullable = true)\n |-- item_id: string (nullable = true)\n |-- item_name: string (nullable = true)\n |-- item_quantity: long (nullable = true)\n |-- item_price: double (nullable = true)\n |-- item_discount: double (nullable = false)\n\n\nSample Data (Processed):\n+--------+--------------------+-----------+---------------------+--------------+--------------+--------------+-------------+--------------+------------------------------------+---------------+-----------------+-------+---------+-------------+----------+-------------+\n|order_id|order_timestamp     |customer_id|customer_email       |customer_fname|customer_lname|customer_mname|customer_city|customer_state|customer_address_struct             |shipping_method|shipping_tracking|item_id|item_name|item_quantity|item_price|item_discount|\n+--------+--------------------+-----------+---------------------+--------------+--------------+--------------+-------------+--------------+------------------------------------+---------------+-----------------+-------+---------+-------------+----------+-------------+\n|ORD-1001|2025-03-28T10:05:00Z|123        |alice.s@example.com  |Alice         |Smith         |NULL          |Anytown      |CA            |{Anytown, CA, 123 Main St, 90210}   |Express        |T123456789       |P-ABC  |Product A|1            |50.0      |0.0          |\n|ORD-1001|2025-03-28T10:05:00Z|123        |alice.s@example.com  |Alice         |Smith         |NULL          |Anytown      |CA            |{Anytown, CA, 123 Main St, 90210}   |Express        |T123456789       |P-XYZ  |Product X|2            |15.5      |0.0          |\n|ORD-1002|2025-03-28T11:15:30Z|456        |bobj@example.net     |Bob           |Jones         |J.            |Otherville   |NY            |{Otherville, NY, 456 Oak Ave, 10001}|NULL           |NULL             |P-LMN  |Product L|1            |120.0     |0.0          |\n|ORD-1003|2025-03-28T12:45:10Z|CUST-789   |charlie.d@example.org|Charlie       |Davis         |NULL          |Anytown      |CA            |{Anytown, CA, 789 Pine Rd, 90211}   |Standard       |T987654321       |P-ABC  |Product A|1            |50.0      |5.0          |\n|ORD-1003|2025-03-28T12:45:10Z|CUST-789   |charlie.d@example.org|Charlie       |Davis         |NULL          |Anytown      |CA            |{Anytown, CA, 789 Pine Rd, 90211}   |Standard       |T987654321       |P-QRS  |Product Q|3            |10.0      |0.0          |\n|ORD-1004|2025-03-28T13:20:00Z|123        |alice.s@example.com  |Alice         |Smith         |NULL          |Anytown      |CA            |{Anytown, CA, 123 Main St, 90210}   |Standard       |NULL             |NULL   |NULL     |NULL         |NULL      |0.0          |\n+--------+--------------------+-----------+---------------------+--------------+--------------+--------------+-------------+--------------+------------------------------------+---------------+-----------------+-------+---------+-------------+----------+-------------+\n\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"878ddf97-1482-408d-b3da-63c288753ae3"},{"cell_type":"markdown","source":["##### Discussion Point: \n","\n","Explain the select choices. Show how dot notation accesses nested fields. Explain explode_outer's role in handling items and empty arrays. Point out how missing fields (middle_name, tracking_no, discount) result in null and how coalesce provides a default. Show the resulting 'flatter' but still potentially wide schema. Discuss the trade-off (more rows due to explode)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"07261896-d490-41ec-a690-de3540162619"},{"cell_type":"markdown","source":["##### Cell 4 (Optional): Alternative Array Handling (e.g., First Item Only)\n","\n","Show this as an _alternative_ if denormalization via explode isn't desired. Explain element_at and size.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b2352af4-f5f7-476d-82f4-f01cf7055370"},{"cell_type":"code","source":["# Alternative if you don't want to explode, e.g., get first item's name\n","if 'raw_df' in locals():\n","    first_item_df = raw_df.select(\n","        F.col(\"order_id\"),\n","        F.element_at(F.col(\"items\"), 1).getField(\"name\").alias(\"first_item_name\"), # Use element_at (1-based index)\n","        F.size(F.col(\"items\")).alias(\"item_count\") # Get array size\n","    )\n","    print(\"\\nSample Data (First Item Name & Count):\")\n","    first_item_df.show()\n","else:\n","    print(\"raw_df not found. Skipping alternative.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"3aab02fa-84d7-4888-8d7d-a9c7f6d144b9","normalized_state":"finished","queued_time":"2025-04-01T21:49:18.0229832Z","session_start_time":null,"execution_start_time":"2025-04-01T21:49:45.1336711Z","execution_finish_time":"2025-04-01T21:49:46.0487937Z","parent_msg_id":"210a6796-de68-4cbb-9a13-20849bb85486"},"text/plain":"StatementMeta(, 3aab02fa-84d7-4888-8d7d-a9c7f6d144b9, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nSample Data (First Item Name & Count):\n+--------+---------------+----------+\n|order_id|first_item_name|item_count|\n+--------+---------------+----------+\n|ORD-1001|      Product A|         2|\n|ORD-1002|      Product L|         1|\n|ORD-1003|      Product A|         2|\n|ORD-1004|           NULL|         0|\n+--------+---------------+----------+\n\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ef74fd01-8d81-4c30-9f31-78b8895bca61"},{"cell_type":"markdown","source":["#### Cell 5: Write Processed Data to Delta Lake"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a7d4c7a-fd5e-486a-8186-18f0b6e90e09"},{"cell_type":"code","source":["# Write the selectively flattened DataFrame to a Delta table in the Lakehouse\n","\n","if 'processed_df' in locals(): # Check if processing ran\n","    try:\n","        processed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(output_delta_table_name)\n","        # Or use .save(f\"Tables/{output_delta_table_name}\") if you prefer path-based\n","        print(f\"Successfully wrote processed data to Delta table: {output_delta_table_name}\")\n","    except Exception as e:\n","        print(f\"Error writing Delta table: {e}\")\n","else:\n","    print(\"processed_df not found. Skipping Delta write.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"3aab02fa-84d7-4888-8d7d-a9c7f6d144b9","normalized_state":"finished","queued_time":"2025-04-01T21:49:18.0244901Z","session_start_time":null,"execution_start_time":"2025-04-01T21:49:46.0511877Z","execution_finish_time":"2025-04-01T21:49:50.8005731Z","parent_msg_id":"69e13116-b890-4e26-851c-f16da2d3bbe1"},"text/plain":"StatementMeta(, 3aab02fa-84d7-4888-8d7d-a9c7f6d144b9, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully wrote processed data to Delta table: orders_processed\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fb7efa8f-6fe9-4a2e-82a3-6dfa5a816181"},{"cell_type":"markdown","source":["##### Discussion Point: \n","\n","Mention the benefits of Delta Lake (ACID, schema evolution, time travel). Show the table appearing in the Lakehouse UI under \"Tables\"."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0b7888f0-8d17-483e-b7a4-dd43f3e2d483"},{"cell_type":"markdown","source":["#### Cell 6: Query via SQL"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ddac550b-4086-4be9-822c-24931015de55"},{"cell_type":"code","source":["%%sql\n","\n","SELECT\n","    order_id,\n","    customer_id,\n","    customer_email,\n","    customer_city,\n","    -- Access fields within the stored struct column\n","    customer_address_struct.street AS customer_street,\n","    customer_address_struct.zip AS customer_zip,\n","    shipping_method,\n","    shipping_tracking,\n","    item_id,\n","    item_name,\n","    item_quantity,\n","    item_price,\n","    item_discount\n","FROM\n","    orders_processed\n","LIMIT 20;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"3aab02fa-84d7-4888-8d7d-a9c7f6d144b9","normalized_state":"finished","queued_time":"2025-04-01T21:49:18.0259765Z","session_start_time":null,"execution_start_time":"2025-04-01T21:49:50.8028371Z","execution_finish_time":"2025-04-01T21:49:56.9230032Z","parent_msg_id":"1e0af8b1-485f-483d-8589-c3067f671356"},"text/plain":"StatementMeta(, 3aab02fa-84d7-4888-8d7d-a9c7f6d144b9, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":8,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"order_id","type":"string","nullable":true,"metadata":{}},{"name":"customer_id","type":"string","nullable":true,"metadata":{}},{"name":"customer_email","type":"string","nullable":true,"metadata":{}},{"name":"customer_city","type":"string","nullable":true,"metadata":{}},{"name":"customer_street","type":"string","nullable":true,"metadata":{}},{"name":"customer_zip","type":"string","nullable":true,"metadata":{}},{"name":"shipping_method","type":"string","nullable":true,"metadata":{}},{"name":"shipping_tracking","type":"string","nullable":true,"metadata":{}},{"name":"item_id","type":"string","nullable":true,"metadata":{}},{"name":"item_name","type":"string","nullable":true,"metadata":{}},{"name":"item_quantity","type":"long","nullable":true,"metadata":{}},{"name":"item_price","type":"double","nullable":true,"metadata":{}},{"name":"item_discount","type":"double","nullable":true,"metadata":{}}]},"data":[["ORD-1001","123","alice.s@example.com","Anytown","123 Main St","90210","Express","T123456789","P-ABC","Product A","1",50,0],["ORD-1001","123","alice.s@example.com","Anytown","123 Main St","90210","Express","T123456789","P-XYZ","Product X","2",15.5,0],["ORD-1002","456","bobj@example.net","Otherville","456 Oak Ave","10001",null,null,"P-LMN","Product L","1",120,0],["ORD-1003","CUST-789","charlie.d@example.org","Anytown","789 Pine Rd","90211","Standard","T987654321","P-ABC","Product A","1",50,5],["ORD-1003","CUST-789","charlie.d@example.org","Anytown","789 Pine Rd","90211","Standard","T987654321","P-QRS","Product Q","3",10,0],["ORD-1004","123","alice.s@example.com","Anytown","123 Main St","90210","Standard",null,null,null,null,null,0]]},"text/plain":"<Spark SQL result set with 6 rows and 13 fields>"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"cad22270-d410-46c0-8214-c86c6f02efff"},{"cell_type":"markdown","source":["##### Discussion Point: \n","\n","Show how standard SQL can query the processed Delta table. Crucially, demonstrate querying inside the customer_address_struct column, proving you retained some structure but made it easily accessible via SQL (also could do this in the SQL endpoint of Lakehouse). Discuss how this balances flattening vs. retaining structure."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5f44b13f-7ad0-4769-b00b-d8d4cdde8425"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"1d3b6cad-9230-4783-9df1-f821711b1c8f","default_lakehouse_name":"HealthcareData","default_lakehouse_workspace_id":"8ecba42e-a6c5-4672-854f-d570b4f45d10"}}},"nbformat":4,"nbformat_minor":5}