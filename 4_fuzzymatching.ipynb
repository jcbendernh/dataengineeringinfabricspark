{"cells":[{"cell_type":"markdown","source":[" #### Fuzzy Matching & Deduplication Across Multiple Columns with a Patient Deduplication scenario for the HealthcareData Lakehouse.\n","\n","Goal: Identify potentially duplicate patient records originating from different sources or containing data entry variations (typos, abbreviations, missing info) using fuzzy matching techniques in a Fabric Spark Notebook.\n","\n","Techniques to Showcase:\n","\n","    1. Data loading and preprocessing/standardization (lowercase, removing punctuation).\n","    2. Blocking strategy (Soundex on name + State) to reduce comparison space.\n","    3. Generating candidate pairs within blocks via self-join.\n","    4. Calculating similarity scores using built-in Spark functions (levenshtein, soundex, datediff).\n","    5. Applying rule-based logic based on multiple field similarities to flag potential duplicates.\n","    6. (Optional) Introduce Pandas UDFs for more advanced fuzzy algorithms (e.g., from thefuzz library).\n","    7. Saving potential duplicate pairs for review.\n","\n","\n","Sample Data:\n","\n"," A single dataset representing raw patient entries from potentially different systems, containing variations that make exact matching difficult.\n","\n","Files/landing/healthcare/patient_raw.csv\n","\n","    Contains raw patient records with variations\n","\n","```mermaid\n","graph TD\n","    A[Raw Patient Data (CSV)] --> B[Data Cleaning & Standardization]\n","    B --> C[Save Cleaned Data as Delta Table]\n","    C --> D[Blocking (SOUNDEX + State)]\n","    D --> E[Candidate Pair Generation (Self-Join on Block Key)]\n","    E --> F[Similarity Scoring (Levenshtein, DOB, Phone)]\n","    F --> G[Apply Matching Rules]\n","    G --> H[Potential Duplicates Table]\n","    H --> I[Query & Review Results]\n","```\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2f8e6f00-ad1f-43f6-9547-07676eaec493"},{"cell_type":"markdown","source":["###### Reset Demo"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e3f70dc4-83f6-48cf-9e5a-e90f9ab387c7"},{"cell_type":"code","source":["%%sql\n","\n","DROP TABLE IF EXISTS patient_cleaned;\n","DROP TABLE IF EXISTS patient_potential_duplicates;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[2,3],"state":"finished","livy_statement_state":"available","session_id":"3327df26-7fae-40ec-8601-7c7ecfce6e61","normalized_state":"finished","queued_time":"2025-04-01T14:15:46.5343086Z","session_start_time":"2025-04-01T14:15:46.5346844Z","execution_start_time":"2025-04-01T14:15:52.4537166Z","execution_finish_time":"2025-04-01T14:16:03.1477727Z","parent_msg_id":"29a98ebb-e3a2-4133-9ad7-61c6a48ee2fb"},"text/plain":"StatementMeta(, 3327df26-7fae-40ec-8601-7c7ecfce6e61, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":1,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":1,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"7ac6d2be-1ee9-4dd8-84cb-aa16516a5501"},{"cell_type":"markdown","source":["#### Cell 1: Setup & Imports"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4ae1e9dd-02ca-4934-aca3-5d8324fc963a"},{"cell_type":"code","source":["# Import PySpark functions\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType, DateType, IntegerType\n","\n","# Optional: For Pandas UDFs later\n","# from pyspark.sql.functions import pandas_udf, PandasUDFType\n","# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n","# import pandas as pd\n","# from thefuzz import fuzz # Requires 'thefuzz' library installed in environment\n","\n","print(\"Setup complete. PySpark functions imported.\")\n","\n","# --- Configuration ---\n","landing_zone_path = \"Files/landing/healthcare\"\n","raw_patient_csv_path = f\"{landing_zone_path}/patient_raw.csv\"\n","\n","# Define Delta table names within the Lakehouse ('Tables' folder)\n","cleaned_patient_table = \"patient_cleaned\"\n","potential_duplicates_table = \"patient_potential_duplicates\"\n","\n","print(f\"Raw Patient CSV Path: {raw_patient_csv_path}\")\n","print(f\"Cleaned Table: {cleaned_patient_table}\")\n","print(f\"Output Duplicates Table: {potential_duplicates_table}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"3327df26-7fae-40ec-8601-7c7ecfce6e61","normalized_state":"finished","queued_time":"2025-04-01T14:16:16.8668999Z","session_start_time":null,"execution_start_time":"2025-04-01T14:16:20.7015934Z","execution_finish_time":"2025-04-01T14:16:21.0428438Z","parent_msg_id":"0083d086-15eb-4d31-a4ad-66fb9431d390"},"text/plain":"StatementMeta(, 3327df26-7fae-40ec-8601-7c7ecfce6e61, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Setup complete. PySpark functions imported.\nRaw Patient CSV Path: Files/landing/healthcare/patient_raw.csv\nCleaned Table: patient_cleaned\nOutput Duplicates Table: patient_potential_duplicates\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a565aae-7f30-422c-a504-f99b008b2ca3"},{"cell_type":"markdown","source":["#### Cell 2: Load and Preprocess Patient Data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c545a4c2-dc7f-4d0d-a959-9ce6449b307e"},{"cell_type":"code","source":["# Load raw data\n","raw_df = spark.read.csv(raw_patient_csv_path, header=True, inferSchema=True)\n","\n","# --- Preprocessing and Standardization ---\n","patients_df = raw_df \\\n","    .withColumn(\"dob\", F.to_date(F.col(\"dob\"), \"yyyy-MM-dd\")) \\\n","    .withColumn(\"first_name_clean\", F.lower(F.trim(F.col(\"first_name\")))) \\\n","    .withColumn(\"last_name_clean\", F.lower(F.trim(F.col(\"last_name\")))) \\\n","    .withColumn(\"street_address_clean\", F.lower(F.trim(F.regexp_replace(\"street_address\", r'[^\\w\\s]', '')))) \\\n","    .withColumn(\"city_clean\", F.lower(F.trim(F.col(\"city\")))) \\\n","    .withColumn(\"state_clean\", F.upper(F.trim(F.col(\"state\")))) \\\n","    .withColumn(\"zip_code_clean\", F.trim(F.col(\"zip_code\"))) \\\n","    .withColumn(\"phone_clean\", F.regexp_replace(F.col(\"phone_number\"), r'[^0-9]', '')) \\\n","    .withColumn(\"full_name_clean\", F.trim(F.concat_ws(\" \", F.col(\"first_name_clean\"), F.col(\"last_name_clean\")))) \\\n","    .fillna(\"\", subset=[\"middle_name\"]) # Replace null middle names with empty string for consistency if needed\n","\n","# Filter out records with essential missing info if necessary (e.g., name or dob)\n","# patients_df = patients_df.filter(F.col(\"last_name_clean\") != \"\" & F.col(\"first_name_clean\") != \"\" & F.col(\"dob\").isNotNull())\n","\n","print(\"Cleaned Patient Data Schema & Sample:\")\n","patients_df.printSchema()\n","patients_df.select(\n","    \"record_id\", \"first_name_clean\", \"last_name_clean\", \"dob\",\n","    \"street_address_clean\", \"city_clean\", \"state_clean\", \"phone_clean\"\n",").show(truncate=False)\n","\n","# Save cleaned data as a Delta table (useful checkpoint)\n","patients_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(cleaned_patient_table)\n","print(f\"Cleaned patient data saved to Delta table: {cleaned_patient_table}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"3327df26-7fae-40ec-8601-7c7ecfce6e61","normalized_state":"finished","queued_time":"2025-04-01T14:16:23.6601445Z","session_start_time":null,"execution_start_time":"2025-04-01T14:16:23.6613274Z","execution_finish_time":"2025-04-01T14:16:29.9996864Z","parent_msg_id":"948dfbfc-db6d-4bb6-8cbb-22532fbf3b9d"},"text/plain":"StatementMeta(, 3327df26-7fae-40ec-8601-7c7ecfce6e61, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cleaned Patient Data Schema & Sample:\nroot\n |-- record_id: string (nullable = true)\n |-- source_system: string (nullable = true)\n |-- mrn: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- middle_name: string (nullable = false)\n |-- dob: date (nullable = true)\n |-- street_address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- zip_code: string (nullable = true)\n |-- phone_number: string (nullable = true)\n |-- first_name_clean: string (nullable = true)\n |-- last_name_clean: string (nullable = true)\n |-- street_address_clean: string (nullable = true)\n |-- city_clean: string (nullable = true)\n |-- state_clean: string (nullable = true)\n |-- zip_code_clean: string (nullable = true)\n |-- phone_clean: string (nullable = true)\n |-- full_name_clean: string (nullable = false)\n\n+---------+----------------+---------------+----------+----------------------+-------------+-----------+-----------+\n|record_id|first_name_clean|last_name_clean|dob       |street_address_clean  |city_clean   |state_clean|phone_clean|\n+---------+----------------+---------------+----------+----------------------+-------------+-----------+-----------+\n|REC001   |john            |smith          |1980-05-20|123 main st           |anytown      |CA         |5551234567 |\n|REC002   |jon             |smith          |1980-05-20|123 main st           |anytown      |CA         |5551234567 |\n|REC003   |john            |smyth          |1980-05-20|123 main street       |anytown      |CA         |15551234567|\n|REC004   |jonathan        |smith          |1980-05-20|123 main st apt 1     |anytown      |CA         |5551234567 |\n|REC005   |alice           |wonder         |1995-11-15|456 oak ave           |otherville   |NY         |5559876543 |\n|REC006   |alice           |wonder         |1995-11-15|456 oak avenue        |otherville   |NY         |5559876543 |\n|REC007   |alice           |wonder         |1995-11-15|456 oak ave 2         |otherville   |NY         |5559876543 |\n|REC008   |robert          |jones          |1975-02-28|789 pine rd           |anytown      |CA         |5551112222 |\n|REC009   |bob             |jones          |1975-02-28|789 pine road         |anytown      |CA         |5551112222 |\n|REC010   |robert          |jones          |1975-02-28|789 pine rd           |anytown      |CA         |5551112222 |\n|REC011   |priya           |patel          |1990-08-01|10 downing st         |london       |UK         |5553334444 |\n|REC012   |charlie         |young          |1988-12-25|1600 amphitheatre pkwy|mountain view|CA         |5555555555 |\n|REC013   |jonn            |smith          |1980-05-20|123 main st           |anytown      |CA         |5551234567 |\n+---------+----------------+---------------+----------+----------------------+-------------+-----------+-----------+\n\nCleaned patient data saved to Delta table: patient_cleaned\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6ea7a09e-1a7b-4509-95f7-a58dfd369e66"},{"cell_type":"markdown","source":["Demo Points: \n","\n","1. Explain the importance of cleaning and standardizing data before matching (lowercase, trim, remove punctuation). \n","2. This is a common task for Bronze to Silver in the Medallion architecture.\n","3. Show the standardized columns. Saving to Delta provides a checkpoint."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9a474db1-7698-4f78-912e-2b93646e7638"},{"cell_type":"markdown","source":["#### Cell 3: Define Blocking Strategy"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59b1fbda-db9f-47f3-b252-110b72ab1fe9"},{"cell_type":"code","source":["# Read cleaned data\n","patients_df = spark.table(cleaned_patient_table)\n","\n","# --- Add Blocking Key ---\n","# Strategy: SOUNDEX of last name + State Abbreviation\n","patients_df = patients_df.withColumn(\n","    \"block_key\", F.concat_ws(\"_\", F.soundex(F.col(\"last_name_clean\")), F.col(\"state_clean\"))\n",")\n","\n","print(\"Data with Blocking Key:\")\n","patients_df.select(\"record_id\", \"last_name_clean\", \"state_clean\", \"block_key\").show()\n","\n","# --- Analyze Block Sizes (Optional but Recommended) ---\n","print(\"\\nBlock Key Distribution:\")\n","block_counts = patients_df.groupBy(\"block_key\").count().orderBy(F.desc(\"count\"))\n","block_counts.show(50)\n","# Check for excessively large blocks which might need a refined blocking key\n","large_blocks = block_counts.filter(\"count > 100\").count() # Adjust threshold as needed\n","print(f\"Number of blocks with size > 100: {large_blocks}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"3327df26-7fae-40ec-8601-7c7ecfce6e61","normalized_state":"finished","queued_time":"2025-04-01T14:16:29.8325295Z","session_start_time":null,"execution_start_time":"2025-04-01T14:16:30.0019281Z","execution_finish_time":"2025-04-01T14:16:38.0339702Z","parent_msg_id":"61fcd418-766e-4bb0-a347-7204dd0aa54d"},"text/plain":"StatementMeta(, 3327df26-7fae-40ec-8601-7c7ecfce6e61, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Data with Blocking Key:\n+---------+---------------+-----------+---------+\n|record_id|last_name_clean|state_clean|block_key|\n+---------+---------------+-----------+---------+\n|   REC001|          smith|         CA|  S530_CA|\n|   REC002|          smith|         CA|  S530_CA|\n|   REC003|          smyth|         CA|  S530_CA|\n|   REC004|          smith|         CA|  S530_CA|\n|   REC005|         wonder|         NY|  W536_NY|\n|   REC006|         wonder|         NY|  W536_NY|\n|   REC007|         wonder|         NY|  W536_NY|\n|   REC008|          jones|         CA|  J520_CA|\n|   REC009|          jones|         CA|  J520_CA|\n|   REC010|          jones|         CA|  J520_CA|\n|   REC011|          patel|         UK|  P340_UK|\n|   REC012|          young|         CA|  Y520_CA|\n|   REC013|          smith|         CA|  S530_CA|\n+---------+---------------+-----------+---------+\n\n\nBlock Key Distribution:\n+---------+-----+\n|block_key|count|\n+---------+-----+\n|  S530_CA|    5|\n|  J520_CA|    3|\n|  W536_NY|    3|\n|  P340_UK|    1|\n|  Y520_CA|    1|\n+---------+-----+\n\nNumber of blocks with size > 100: 0\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8b710a48-85c4-4253-ad0c-c101f7e3dfd6"},{"cell_type":"markdown","source":["Demo Point: Explain the purpose of blocking – to reduce the number of comparisons from N*N to manageable sizes. Discuss the chosen strategy (Soundex + State) and why it helps group potentially similar records. Show the block key distribution; ideally, most blocks should be relatively small.\n","\n","Talking points: \n","\n","- \"Imagine finding duplicate patients in millions of healthcare records. Comparing every single record to every other record looking for typos or variations is computationally impossible – it would take forever.\"\n","- \"Blocking is the smart shortcut. For our patient data, we first quickly group records using a specific strategy: grouping by how the last name sounds (using an algorithm called Soundex) combined with the patient's State.\"\n","- \"Why this combo? Soundex catches common phonetic typos (like 'Smith' vs. 'Smyth'), while adding the State drastically narrows down the possibilities – we only compare similar-sounding names within the same state.\"\n","- \"Then, we only run the detailed, slower fuzzy comparison on these much smaller, targeted groups. It dramatically cuts down the workload, making large-scale patient deduplication fast and efficient.\"\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0c21be4d-9ca6-4d71-b791-bc55c098f962"},{"cell_type":"markdown","source":["#### Cell 4: Generate Candidate Pairs within Blocks"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4fbec57a-065e-410f-95ab-d6b10b7cf373"},{"cell_type":"code","source":["# --- Generate Candidate Pairs by Self-Joining on Block Key ---\n","df1 = patients_df.alias(\"df1\")\n","df2 = patients_df.alias(\"df2\")\n","\n","# Join on block_key and ensure we compare different records only once\n","candidate_pairs_df = df1.join(\n","    df2,\n","    on=(F.col(\"df1.block_key\") == F.col(\"df2.block_key\")) & (F.col(\"df1.record_id\") < F.col(\"df2.record_id\")),\n","    how=\"inner\"\n",")\n","\n","# Select relevant columns for comparison\n","candidate_pairs_df = candidate_pairs_df.select(\n","    F.col(\"df1.record_id\").alias(\"id1\"),\n","    F.col(\"df1.full_name_clean\").alias(\"name1\"),\n","    F.col(\"df1.dob\").alias(\"dob1\"),\n","    F.col(\"df1.street_address_clean\").alias(\"address1\"),\n","    F.col(\"df1.city_clean\").alias(\"city1\"),\n","    F.col(\"df1.state_clean\").alias(\"state1\"),\n","    F.col(\"df1.zip_code_clean\").alias(\"zip1\"),\n","    F.col(\"df1.phone_clean\").alias(\"phone1\"),\n","    F.col(\"df2.record_id\").alias(\"id2\"),\n","    F.col(\"df2.full_name_clean\").alias(\"name2\"),\n","    F.col(\"df2.dob\").alias(\"dob2\"),\n","    F.col(\"df2.street_address_clean\").alias(\"address2\"),\n","    F.col(\"df2.city_clean\").alias(\"city2\"),\n","    F.col(\"df2.state_clean\").alias(\"state2\"),\n","    F.col(\"df2.zip_code_clean\").alias(\"zip2\"),\n","    F.col(\"df2.phone_clean\").alias(\"phone2\")\n",")\n","\n","print(f\"\\nGenerated {candidate_pairs_df.count()} candidate pairs for comparison.\")\n","print(\"Sample Candidate Pairs:\")\n","candidate_pairs_df.show(5, truncate=False)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"3327df26-7fae-40ec-8601-7c7ecfce6e61","normalized_state":"finished","queued_time":"2025-04-01T14:16:43.7240279Z","session_start_time":null,"execution_start_time":"2025-04-01T14:16:43.7252604Z","execution_finish_time":"2025-04-01T14:16:46.0872548Z","parent_msg_id":"87c27ea5-dd70-42a7-bd21-ce9eb785ecd3"},"text/plain":"StatementMeta(, 3327df26-7fae-40ec-8601-7c7ecfce6e61, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nGenerated 16 candidate pairs for comparison.\nSample Candidate Pairs:\n+------+----------+----------+-----------+-------+------+-----+----------+------+--------------+----------+-----------------+-------+------+-----+-----------+\n|id1   |name1     |dob1      |address1   |city1  |state1|zip1 |phone1    |id2   |name2         |dob2      |address2         |city2  |state2|zip2 |phone2     |\n+------+----------+----------+-----------+-------+------+-----+----------+------+--------------+----------+-----------------+-------+------+-----+-----------+\n|REC001|john smith|1980-05-20|123 main st|anytown|CA    |90210|5551234567|REC013|jonn smith    |1980-05-20|123 main st      |anytown|CA    |90210|5551234567 |\n|REC001|john smith|1980-05-20|123 main st|anytown|CA    |90210|5551234567|REC004|jonathan smith|1980-05-20|123 main st apt 1|anytown|CA    |90210|5551234567 |\n|REC001|john smith|1980-05-20|123 main st|anytown|CA    |90210|5551234567|REC003|john smyth    |1980-05-20|123 main street  |anytown|CA    |90210|15551234567|\n|REC001|john smith|1980-05-20|123 main st|anytown|CA    |90210|5551234567|REC002|jon smith     |1980-05-20|123 main st      |anytown|CA    |90210|5551234567 |\n|REC002|jon smith |1980-05-20|123 main st|anytown|CA    |90210|5551234567|REC013|jonn smith    |1980-05-20|123 main st      |anytown|CA    |90210|5551234567 |\n+------+----------+----------+-----------+-------+------+-----+----------+------+--------------+----------+-----------------+-------+------+-----+-----------+\nonly showing top 5 rows\n\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8d201e60-234d-4f04-b90f-73a922d426fd"},{"cell_type":"markdown","source":["Demo Point: Explain the self-join on the block_key. Emphasize the df1.record_id < df2.record_id condition to prevent self-comparisons and duplicate pairs. Show the resulting candidate pairs ready for scoring.\n","\n","Talking points:\n","\n","- \"Alright, we've now grouped potentially similar patients into 'blocks' using Soundex and State.\"\n","- \"The next step is crucial: creating the actual pairs of patients within each block that we need to compare closely. We do this efficiently using a 'self-join' – essentially, matching our patient list back to itself only when the block keys match.\"\n","- \"The really clever part is adding one simple condition: Record ID 1 < Record ID 2. This instantly prevents two things: comparing a patient record to itself (which is pointless) and creating duplicate pairs (like comparing Alice to Bob and Bob to Alice – we only need it once!).\"\n","- \"So, this step quickly and efficiently gives us just the unique 'candidate pairs' we actually need to analyze, ready for the detailed fuzzy scoring in the next stage.\"\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70e9235d-034c-4c52-9f6b-56ee0fcae663"},{"cell_type":"markdown","source":["#### Cell 5: Calculate Similarities & Score Pairs (Rule-Based)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"091058a1-0dc4-4cd0-8f2e-2c4919fef3f2"},{"cell_type":"code","source":["# --- Calculate Similarity Scores ---\n","scored_pairs_df = candidate_pairs_df.withColumn(\n","    # Lower score = more similar for Levenshtein distance\n","    \"name_lev\", F.levenshtein(F.col(\"name1\"), F.col(\"name2\"))\n",").withColumn(\n","    # Difference in days for DOB. Lower = more similar.\n","    \"dob_diff\", F.abs(F.datediff(F.col(\"dob1\"), F.col(\"dob2\")))\n",").withColumn(\n","    # Levenshtein on street address\n","    \"address_lev\", F.levenshtein(F.col(\"address1\"), F.col(\"address2\"))\n",").withColumn(\n","    # Levenshtein on cleaned phone number\n","    \"phone_lev\", F.levenshtein(F.col(\"phone1\"), F.col(\"phone2\"))\n",")\n","\n","print(\"\\nPairs with Similarity Scores:\")\n","scored_pairs_df.select(\"id1\", \"name1\", \"id2\", \"name2\", \"name_lev\", \"dob_diff\", \"address_lev\", \"phone_lev\").show(truncate=False)\n","\n","# --- Apply Matching Rules ---\n","# Example rules (adjust thresholds based on data and requirements):\n","# Rule 1: Very similar name, exact DOB -> Potential Match\n","# Rule 2: Exact name, similar address, close DOB -> Potential Match\n","# Rule 3: Similar name, exact DOB, similar phone -> Potential Match\n","\n","match_threshold_name_close = 3\n","match_threshold_name_exact = 0\n","match_threshold_dob_exact = 0\n","match_threshold_dob_close = 7 # Allow 1 week difference? Or less.\n","match_threshold_address_close = 5\n","match_threshold_phone_close = 2\n","\n","scored_pairs_df = scored_pairs_df.withColumn(\n","    \"is_potential_duplicate\",\n","    F.when(\n","        (F.col(\"name_lev\") <= match_threshold_name_close) &\n","        (F.col(\"dob_diff\") <= match_threshold_dob_exact),\n","        True\n","    ).when(\n","        (F.col(\"name_lev\") <= match_threshold_name_exact) &\n","        (F.col(\"address_lev\") <= match_threshold_address_close) &\n","        (F.col(\"dob_diff\") <= match_threshold_dob_close),\n","        True\n","    ).when(\n","        (F.col(\"name_lev\") <= match_threshold_name_close) &\n","        (F.col(\"dob_diff\") <= match_threshold_dob_exact) &\n","        (F.col(\"phone_lev\") <= match_threshold_phone_close),\n","        True\n","    ).otherwise(False)\n",")\n","\n","print(\"\\nPairs with Potential Duplicate Flag:\")\n","scored_pairs_df.select(\n","    \"id1\", \"name1\", \"id2\", \"name2\", \"name_lev\", \"dob_diff\",\n","    \"address_lev\", \"phone_lev\", \"is_potential_duplicate\"\n",").show(truncate=False)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"3327df26-7fae-40ec-8601-7c7ecfce6e61","normalized_state":"finished","queued_time":"2025-04-01T14:16:57.0957733Z","session_start_time":null,"execution_start_time":"2025-04-01T14:16:57.097057Z","execution_finish_time":"2025-04-01T14:16:58.6868509Z","parent_msg_id":"92bb62bb-502b-497e-9c40-8bebcf0534c5"},"text/plain":"StatementMeta(, 3327df26-7fae-40ec-8601-7c7ecfce6e61, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nPairs with Similarity Scores:\n+------+--------------+------+--------------+--------+--------+-----------+---------+\n|id1   |name1         |id2   |name2         |name_lev|dob_diff|address_lev|phone_lev|\n+------+--------------+------+--------------+--------+--------+-----------+---------+\n|REC001|john smith    |REC013|jonn smith    |1       |0       |0          |0        |\n|REC001|john smith    |REC004|jonathan smith|4       |0       |6          |0        |\n|REC001|john smith    |REC003|john smyth    |1       |0       |4          |1        |\n|REC001|john smith    |REC002|jon smith     |1       |0       |0          |0        |\n|REC002|jon smith     |REC013|jonn smith    |1       |0       |0          |0        |\n|REC002|jon smith     |REC004|jonathan smith|5       |0       |6          |0        |\n|REC002|jon smith     |REC003|john smyth    |2       |0       |4          |1        |\n|REC003|john smyth    |REC013|jonn smith    |2       |0       |4          |1        |\n|REC003|john smyth    |REC004|jonathan smith|5       |0       |5          |1        |\n|REC004|jonathan smith|REC013|jonn smith    |4       |0       |6          |0        |\n|REC005|alice wonder  |REC007|alice wonder  |0       |0       |2          |0        |\n|REC005|alice wonder  |REC006|alice wonder  |0       |0       |3          |0        |\n|REC006|alice wonder  |REC007|alice wonder  |0       |0       |3          |0        |\n|REC008|robert jones  |REC010|robert jones  |0       |0       |0          |0        |\n|REC008|robert jones  |REC009|bob jones     |4       |0       |2          |0        |\n|REC009|bob jones     |REC010|robert jones  |4       |0       |2          |0        |\n+------+--------------+------+--------------+--------+--------+-----------+---------+\n\n\nPairs with Potential Duplicate Flag:\n+------+--------------+------+--------------+--------+--------+-----------+---------+----------------------+\n|id1   |name1         |id2   |name2         |name_lev|dob_diff|address_lev|phone_lev|is_potential_duplicate|\n+------+--------------+------+--------------+--------+--------+-----------+---------+----------------------+\n|REC001|john smith    |REC013|jonn smith    |1       |0       |0          |0        |true                  |\n|REC001|john smith    |REC004|jonathan smith|4       |0       |6          |0        |false                 |\n|REC001|john smith    |REC003|john smyth    |1       |0       |4          |1        |true                  |\n|REC001|john smith    |REC002|jon smith     |1       |0       |0          |0        |true                  |\n|REC002|jon smith     |REC013|jonn smith    |1       |0       |0          |0        |true                  |\n|REC002|jon smith     |REC004|jonathan smith|5       |0       |6          |0        |false                 |\n|REC002|jon smith     |REC003|john smyth    |2       |0       |4          |1        |true                  |\n|REC003|john smyth    |REC013|jonn smith    |2       |0       |4          |1        |true                  |\n|REC003|john smyth    |REC004|jonathan smith|5       |0       |5          |1        |false                 |\n|REC004|jonathan smith|REC013|jonn smith    |4       |0       |6          |0        |false                 |\n|REC005|alice wonder  |REC007|alice wonder  |0       |0       |2          |0        |true                  |\n|REC005|alice wonder  |REC006|alice wonder  |0       |0       |3          |0        |true                  |\n|REC006|alice wonder  |REC007|alice wonder  |0       |0       |3          |0        |true                  |\n|REC008|robert jones  |REC010|robert jones  |0       |0       |0          |0        |true                  |\n|REC008|robert jones  |REC009|bob jones     |4       |0       |2          |0        |false                 |\n|REC009|bob jones     |REC010|robert jones  |4       |0       |2          |0        |false                 |\n+------+--------------+------+--------------+--------+--------+-----------+---------+----------------------+\n\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc7983f7-c215-4018-a7d9-47e2b062bfbe"},{"cell_type":"markdown","source":["Demo Point: Explain the use of levenshtein for string similarity (lower is better) and datediff for dates. Show the calculated scores. Explain the simple rule-based logic combining thresholds across multiple fields using when/otherwise to flag potential duplicates. Discuss how these thresholds would be tuned.\n","\n","Talking points:\n","\n","- \"So, we have our 'candidate pairs' – records that might be duplicates because they were in the same block. Now, we need to score how similar they truly are.\"\n","- \"For text like names and addresses, we're using a function called 'Levenshtein distance'. It basically counts the minimum 'typos' or edits needed to make one string match the other – the key is, a lower Levenshtein score means a better, more similar match.\"\n","- \"For dates of birth, we simply calculate the difference in days – again, closer to zero is better.\"\n","- \"But just one similar field isn't usually enough proof. So, we combine these scores using simple 'WHEN...THEN' rules: for example, 'WHEN the name similarity score is very low AND the date difference is zero or tiny, THEN flag this pair as a potential duplicate'. We can add other rules combining different fields.\"\n","- \"Crucially, those 'very low' thresholds are adjustable – we'd tune them based on analyzing the data here in St. Louis and deciding how strict we need to be. This step flags the most promising pairs for review.\"\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"df25b297-314e-415d-847a-04aa7c3c77e6"},{"cell_type":"markdown","source":["#### Cell 6 (Optional): Advanced Scoring with Pandas UDF"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0216549e-0861-40e9-b081-46ed5bfe635e"},{"cell_type":"code","source":["# --- Optional: Advanced scoring using 'thefuzz' library via Pandas UDF ---\n","# Ensure 'thefuzz' is installed in your Fabric Spark environment.\n","\n","# # Example Schema for UDF output\n","# score_schema = StructType([\n","#     StructField(\"name_ratio\", IntegerType()),\n","#     StructField(\"name_partial_ratio\", IntegerType()),\n","#     StructField(\"name_token_sort_ratio\", IntegerType()),\n","#     StructField(\"address_ratio\", IntegerType())\n","# ])\n","\n","# # Define the Pandas UDF\n","# @pandas_udf(score_schema, PandasUDFType.SCALAR)\n","# def calculate_fuzzy_scores_udf(name1_series: pd.Series, name2_series: pd.Series, address1_series: pd.Series, address2_series: pd.Series) -> pd.DataFrame:\n","#     \"\"\"Calculates various fuzzy scores for pairs.\"\"\"\n","#     results = []\n","#     for name1, name2, address1, address2 in zip(name1_series, name2_series, address1_series, address2_series):\n","#         name_r = fuzz.ratio(name1, name2)\n","#         name_pr = fuzz.partial_ratio(name1, name2)\n","#         name_tsr = fuzz.token_sort_ratio(name1, name2)\n","#         address_r = fuzz.ratio(address1, address2)\n","#         results.append((name_r, name_pr, name_tsr, address_r))\n","#     return pd.DataFrame(results, columns=[\"name_ratio\", \"name_partial_ratio\", \"name_token_sort_ratio\", \"address_ratio\"])\n","\n","# # Apply the UDF\n","# # Note: Ensure candidate_pairs_df is available if running this cell\n","# scored_pairs_adv_df = candidate_pairs_df.withColumn(\n","#     \"fuzzy_scores\",\n","#     calculate_fuzzy_scores_udf(\n","#         F.col(\"name1\"), F.col(\"name2\"), F.col(\"address1\"), F.col(\"address2\")\n","#     )\n","# ).select(\"*\", \"fuzzy_scores.*\").drop(\"fuzzy_scores\") # Expand the struct\n","\n","# print(\"\\nPairs with Advanced Fuzzy Scores (using thefuzz):\")\n","# scored_pairs_adv_df.select(\n","#     \"id1\", \"name1\", \"id2\", \"name2\",\n","#     \"name_ratio\", \"name_partial_ratio\", \"name_token_sort_ratio\", \"address_ratio\"\n","# ).show(truncate=False)\n","\n","# # You could then define 'is_potential_duplicate' based on these richer scores, e.g.,\n","# # high token_sort_ratio for name AND high address_ratio AND close DOB"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"3327df26-7fae-40ec-8601-7c7ecfce6e61","normalized_state":"finished","queued_time":"2025-04-01T14:17:11.6889766Z","session_start_time":null,"execution_start_time":"2025-04-01T14:17:11.6902424Z","execution_finish_time":"2025-04-01T14:17:12.1248628Z","parent_msg_id":"f2566d75-80b7-4dff-ba8b-aef77cd63daf"},"text/plain":"StatementMeta(, 3327df26-7fae-40ec-8601-7c7ecfce6e61, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cf368b62-9ce3-4464-8ae0-313172ed7982"},{"cell_type":"markdown","source":["Demo Point: Briefly explain that for more complex matching, Pandas UDFs allow using powerful Python libraries like thefuzz. Show the structure (define schema, define UDF, apply UDF). Mention higher scores (like ratio) mean more similar, unlike Levenshtein distance. Keep this optional or brief unless the audience is very technical.    "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20b6d83b-27de-401b-82bc-b35162716690"},{"cell_type":"markdown","source":["#### Cell 7: Filter, Show, and Save Potential Duplicates"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"398978b6-945b-45a4-a312-8aca870e99ae"},{"cell_type":"code","source":["# --- Filter for potential duplicates based on the rules ---\n","potential_duplicates_df = scored_pairs_df.filter(F.col(\"is_potential_duplicate\") == True)\n","\n","print(f\"\\nFound {potential_duplicates_df.count()} potential duplicate pairs.\")\n","print(\"Potential Duplicate Pairs Details:\")\n","\n","# Select a clean set of columns to display/save\n","display_cols = [\n","    \"id1\", \"name1\", \"dob1\", \"address1\", \"city1\", \"state1\", \"zip1\", \"phone1\",\n","    \"id2\", \"name2\", \"dob2\", \"address2\", \"city2\", \"state2\", \"zip2\", \"phone2\",\n","    \"name_lev\", \"dob_diff\", \"address_lev\", \"phone_lev\" # Include scores for context\n","]\n","potential_duplicates_to_show = potential_duplicates_df.select(display_cols)\n","potential_duplicates_to_show.show(truncate=False)\n","\n","# --- Save the potential duplicate pairs to a Delta table ---\n","try:\n","    potential_duplicates_to_show.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(potential_duplicates_table)\n","    print(f\"Potential duplicate pairs saved to Delta table: {potential_duplicates_table}\")\n","except Exception as e:\n","    print(f\"Error saving potential duplicates: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"3327df26-7fae-40ec-8601-7c7ecfce6e61","normalized_state":"finished","queued_time":"2025-04-01T14:17:15.9908308Z","session_start_time":null,"execution_start_time":"2025-04-01T14:17:15.9920469Z","execution_finish_time":"2025-04-01T14:17:20.7069779Z","parent_msg_id":"3c39c65e-2cb7-4659-93e0-62b53a3f7285"},"text/plain":"StatementMeta(, 3327df26-7fae-40ec-8601-7c7ecfce6e61, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nFound 10 potential duplicate pairs.\nPotential Duplicate Pairs Details:\n+------+------------+----------+---------------+----------+------+-----+-----------+------+------------+----------+---------------+----------+------+-----+-----------+--------+--------+-----------+---------+\n|id1   |name1       |dob1      |address1       |city1     |state1|zip1 |phone1     |id2   |name2       |dob2      |address2       |city2     |state2|zip2 |phone2     |name_lev|dob_diff|address_lev|phone_lev|\n+------+------------+----------+---------------+----------+------+-----+-----------+------+------------+----------+---------------+----------+------+-----+-----------+--------+--------+-----------+---------+\n|REC001|john smith  |1980-05-20|123 main st    |anytown   |CA    |90210|5551234567 |REC013|jonn smith  |1980-05-20|123 main st    |anytown   |CA    |90210|5551234567 |1       |0       |0          |0        |\n|REC001|john smith  |1980-05-20|123 main st    |anytown   |CA    |90210|5551234567 |REC003|john smyth  |1980-05-20|123 main street|anytown   |CA    |90210|15551234567|1       |0       |4          |1        |\n|REC001|john smith  |1980-05-20|123 main st    |anytown   |CA    |90210|5551234567 |REC002|jon smith   |1980-05-20|123 main st    |anytown   |CA    |90210|5551234567 |1       |0       |0          |0        |\n|REC002|jon smith   |1980-05-20|123 main st    |anytown   |CA    |90210|5551234567 |REC013|jonn smith  |1980-05-20|123 main st    |anytown   |CA    |90210|5551234567 |1       |0       |0          |0        |\n|REC002|jon smith   |1980-05-20|123 main st    |anytown   |CA    |90210|5551234567 |REC003|john smyth  |1980-05-20|123 main street|anytown   |CA    |90210|15551234567|2       |0       |4          |1        |\n|REC003|john smyth  |1980-05-20|123 main street|anytown   |CA    |90210|15551234567|REC013|jonn smith  |1980-05-20|123 main st    |anytown   |CA    |90210|5551234567 |2       |0       |4          |1        |\n|REC005|alice wonder|1995-11-15|456 oak ave    |otherville|NY    |10001|5559876543 |REC007|alice wonder|1995-11-15|456 oak ave 2  |otherville|NY    |10001|5559876543 |0       |0       |2          |0        |\n|REC005|alice wonder|1995-11-15|456 oak ave    |otherville|NY    |10001|5559876543 |REC006|alice wonder|1995-11-15|456 oak avenue |otherville|NY    |10001|5559876543 |0       |0       |3          |0        |\n|REC006|alice wonder|1995-11-15|456 oak avenue |otherville|NY    |10001|5559876543 |REC007|alice wonder|1995-11-15|456 oak ave 2  |otherville|NY    |10001|5559876543 |0       |0       |3          |0        |\n|REC008|robert jones|1975-02-28|789 pine rd    |anytown   |CA    |90211|5551112222 |REC010|robert jones|1975-02-28|789 pine rd    |anytown   |CA    |90211|5551112222 |0       |0       |0          |0        |\n+------+------------+----------+---------------+----------+------+-----+-----------+------+------------+----------+---------------+----------+------+-----+-----------+--------+--------+-----------+---------+\n\nPotential duplicate pairs saved to Delta table: patient_potential_duplicates\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bfdf03a0-5545-4867-9db4-34c3ce8b8836"},{"cell_type":"markdown","source":["Demo Point: Show the final filtered list of pairs flagged as potential duplicates. Explain that this output would typically feed into a review process or a more advanced clustering step. Save the results."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d38ec7cd-8d9e-44e9-b661-81aea92be7b5"},{"cell_type":"markdown","source":["#### Cell 8: Query Results & Discussion"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c0a4f6a5-10f3-472b-a566-126323459e60"},{"cell_type":"code","source":["%%sql\n","-- Query the potential duplicates table\n","SELECT\n","    id1, name1, dob1, address1,\n","    id2, name2, dob2, address2,\n","    name_lev, dob_diff -- Show similarity scores\n","FROM\n","    patient_potential_duplicates -- Use table name directly\n","ORDER BY name1, id1, id2;"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"3327df26-7fae-40ec-8601-7c7ecfce6e61","normalized_state":"finished","queued_time":"2025-04-01T14:17:23.7442169Z","session_start_time":null,"execution_start_time":"2025-04-01T14:17:23.7453685Z","execution_finish_time":"2025-04-01T14:17:27.1064646Z","parent_msg_id":"1990a3b0-b2d8-4ed8-b611-ebf254a5944f"},"text/plain":"StatementMeta(, 3327df26-7fae-40ec-8601-7c7ecfce6e61, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":9,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"id1","type":"string","nullable":true,"metadata":{}},{"name":"name1","type":"string","nullable":true,"metadata":{}},{"name":"dob1","type":"date","nullable":true,"metadata":{}},{"name":"address1","type":"string","nullable":true,"metadata":{}},{"name":"id2","type":"string","nullable":true,"metadata":{}},{"name":"name2","type":"string","nullable":true,"metadata":{}},{"name":"dob2","type":"date","nullable":true,"metadata":{}},{"name":"address2","type":"string","nullable":true,"metadata":{}},{"name":"name_lev","type":"integer","nullable":true,"metadata":{}},{"name":"dob_diff","type":"integer","nullable":true,"metadata":{}}]},"data":[["REC005","alice wonder","1995-11-15","456 oak ave","REC006","alice wonder","1995-11-15","456 oak avenue",0,0],["REC005","alice wonder","1995-11-15","456 oak ave","REC007","alice wonder","1995-11-15","456 oak ave 2",0,0],["REC006","alice wonder","1995-11-15","456 oak avenue","REC007","alice wonder","1995-11-15","456 oak ave 2",0,0],["REC001","john smith","1980-05-20","123 main st","REC002","jon smith","1980-05-20","123 main st",1,0],["REC001","john smith","1980-05-20","123 main st","REC003","john smyth","1980-05-20","123 main street",1,0],["REC001","john smith","1980-05-20","123 main st","REC013","jonn smith","1980-05-20","123 main st",1,0],["REC003","john smyth","1980-05-20","123 main street","REC013","jonn smith","1980-05-20","123 main st",2,0],["REC002","jon smith","1980-05-20","123 main st","REC003","john smyth","1980-05-20","123 main street",2,0],["REC002","jon smith","1980-05-20","123 main st","REC013","jonn smith","1980-05-20","123 main st",1,0],["REC008","robert jones","1975-02-28","789 pine rd","REC010","robert jones","1975-02-28","789 pine rd",0,0]]},"text/plain":"<Spark SQL result set with 10 rows and 10 fields>"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"52e334d5-ef9f-4c92-be52-ff81a3fcddca"},{"cell_type":"markdown","source":["Demo Point: Show querying the results using SQL. Discuss next steps: Manual review, using graph algorithms (like Connected Components on the pairs) to group records belonging to the same entity, assigning a master ID (Golden Record). Emphasize that this demo focused on identifying pairs, and resolving them is often the subsequent step.\n","\n","Talking Points:\n","\n","- \"So, right here in the notebook, using standard SQL, we can immediately query the potential duplicate patient pairs our fuzzy logic just flagged – you can see the different record IDs side-by-side, along with the similarity scores that triggered the match.\"\n","- \"This output table, stored right here in our Fabric Lakehouse, is the crucial input for the next phase. Typically, these potential duplicates would either go to data stewards for review, or more powerfully, you'd feed these pairs into graph algorithms, like Connected Components.\"\n","- \"Think of it like connecting the dots – if the algorithm sees 'Record A matches B' and 'Record B matches C', it automatically groups A, B, and C together as likely the same person.\"\n","- \"The ultimate goal is usually assigning one unique Master Patient ID or creating a single 'Golden Record' for each individual, essential for accurate care and reporting right here for our patients in St. Louis.\"\n","- \"What we focused on in this demo was that vital, complex first step: using Spark's power to accurately identify the potential duplicate pairs from messy data, paving the way for creating that clean, trusted patient view.\"\n","\n","Technical points:\n","\n","name_lev: This  stands for \"Name Levenshtein distance\".\n","\n","    It's calculated using the levenshtein() Spark function between the cleaned full names of the two patient records being compared (name1 and name2).\n","    What it means: Levenshtein distance measures the minimum number of single-character edits (insertions, deletions, or substitutions) needed to change one 1 name into the other. \n","\n","Levenshtein distance - Wikipedia\n","\n","https://en.wikipedia.org/wiki/Levenshtein_distance\n","\n","        Interpretation: A lower name_lev score means the names are more similar. A score of 0 means the names are identical. A small score (like 1 or 2) indicates a minor typo or difference (e.g., \"Jon\" vs \"John\", \"Smith\" vs \"Smyth\").\n","\n","dob_diff: This stands for \"Date of Birth Difference\".\n","        It's calculated using the abs(datediff(dob1, dob2)) Spark functions between the dates of birth of the two patient records (dob1 and dob2).\n","        What it means: It measures the absolute difference between the two dates of birth, expressed in days.\n","        Interpretation: A lower dob_diff score means the dates of birth are closer together. A score of 0 means the DOBs are identical. A small score might indicate a minor data entry error, while a large score indicates they are definitely different birth dates.\n","\n","In the demo, these scores are used together in rules (like WHEN name_lev <= 3 AND dob_diff <= 1 THEN...) to decide if a pair of records is similar enough across multiple fields to be flagged as a potential duplicate.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c45e4836-ac67-4d8e-ab86-a5987a4b4475"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3960667a-4880-4a76-a6f0-9c1f815302d5"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"1d3b6cad-9230-4783-9df1-f821711b1c8f","default_lakehouse_name":"HealthcareData","default_lakehouse_workspace_id":"8ecba42e-a6c5-4672-854f-d570b4f45d10"}}},"nbformat":4,"nbformat_minor":5}